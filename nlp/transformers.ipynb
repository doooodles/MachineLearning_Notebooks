{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"transformers.ipynb","provenance":[],"collapsed_sections":["swDUGQDm7dfe","SmfNEG-_1j0r"],"mount_file_id":"1dAycfA3ldhHktD80zKUNXalR0QuSCadC","authorship_tag":"ABX9TyML7lHJO6R47ZXtQY3BuaWC"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"swDUGQDm7dfe"},"source":["# Transformers\n","\n","Paper: [Attention is all you need](https://arxiv.org/abs/1706.03762)\n","\n","Resources:\n","- [The Illustrated Transformer\n","](https://jalammar.github.io/illustrated-transformer/)\n","- [Visualizing A Neural Machine Translation Model (Mechanics of Seq2seq Models With Attention)](https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/)\n","- [TF Docs: Transformer model for language understanding\n","](https://www.tensorflow.org/text/tutorials/transformer)\n","\n","#### Summary\n","- Network based only on attention, without recurrence or convolutions.\n","- Addresses long sequence dependence problem of recurrent models with single encoder hidden state.\n","- Position information is included separately \n","\n","### Approach\n","\n","- Until done:\n","    1. Read the paper\n","    2. Try to implement until stuck\n","    3. Check resources\n","    4. Go to 1.\n","The end product will look a lot like the TF example, *but* I'll learn a lot along the way.\n","\n","### Results\n","- Tried both Portugese to English from the TF docs and Russian to English.\n","- The Russian to English did not perform as well given the time allocated for training and may need additional hyperparameter tuning.\n","\n","[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tims457/ml_notebooks/blob/main/nlp/transformers.ipynb)\n"]},{"cell_type":"code","metadata":{"id":"CGY6wp3dAJOq"},"source":["import tensorflow as tf\n","import numpy as np\n","import tensorflow_datasets as tfds\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8Zg0LYfqACrO"},"source":["# Attention model\n","\n","### Notation\n","- $Q$ = Query\n","- $K$ = Key, dimension $d_k$ = 64\n","- $V$ = Value, dimension $d_v$ = 64\n","- $n$ = input sequence length\n","- $m$ = output sequence length\n","- $d_\\text{model}$ = model dimension = 512\n","- $d_{ff}$ = inner layer dimension = 2048\n","- $h$ = heads\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Yt2vmmu7Ntjy"},"source":["## Attention\n","\n","### Basics\n","- Deals with problem of relationships between elements of a long sequence experienced by recurrent models.\n","- Pass all hidden states (all encoder output) to decoder.\n","- Decoder scores each of the encoder hidden states, applies a softmax to the score, then multiplies the score times the hidden state from the encoder. This increases the value of states that scored higher.\n","- Rather than having a single hidden state from a recurrent sequence, the decoder has *all* the hidden states and a score (or attention) value for each of them to weight their importance.\n","- Self-attention means the network is learning to associate the relationships between input elements. \n","\n","<img src=\"https://www.researchgate.net/publication/333078019/figure/fig1/AS:758304078839808@1557805189409/left-Scaled-Dot-Product-Attention-right-Multi-Head-Attention.png\" height=300 />"]},{"cell_type":"markdown","metadata":{"id":"N9EJUz9cK5fq"},"source":["## Multi-head attention\n","- Query, Key, Value (Q,K,V) for each of the encoder input vector created by the embedding.\n","- Attention value is the softmax of Query x Key scaled by the dimension $d_k$ then multiplied by Value\n","\n","$$\\text{Attention(Q,K,V)} = \\text{softmax}_k(\\frac{QK^T}{\\sqrt{d_k}})V$$\n","\n","- Multiple heads - number of attention layers running in parallel\n","- By starting with multiple heads (8 in this case) the encoder and decoder are really multiple encoders and decoders operating in parallel. \n","\n"]},{"cell_type":"markdown","metadata":{"id":"19f35FemNUHx"},"source":[""]},{"cell_type":"code","metadata":{"id":"YdGIapvC0VNx"},"source":["import tensorflow as tf\n","from tensorflow.keras import layers\n","\n","# Attention\n","# sec 3.2.1\n","def scaled_dot_product_attention(q, k, v, mask):\n","    # get dimensions of the input, cast from tensor to float\n","    d_k = tf.cast(tf.shape(k)[-1], tf.float32)\n","    \n","    # compute queries x keys and scale by dimension\n","    attention_logits = tf.matmul(q, k, transpose_b=True)\n","    \n","    scaled_attention_logits = attention_logits / tf.math.sqrt(d_k)\n","    # print(f\"scaled attention shape {scaled_attention_logits.shape}\")\n","\n","    # apply decoder mask\n","    if mask is not None:\n","        scaled_attention_logits += (mask * -1e9)\n","\n","    # normalize all scores\n","    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n","    # print(f\"attention shape {attention_weights.shape}\")\n","\n","    # times value\n","    output = tf.matmul(attention_weights, v)\n","    # print(f\"output shape {output.shape}\")\n","\n","    return output, attention_weights"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fq0feSF1aa-o","executionInfo":{"status":"ok","timestamp":1624123270577,"user_tz":360,"elapsed":63,"user":{"displayName":"Tim Sullivan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgDOmSXGsobJcA9UOtHRwU-sWjSZffSAqqvuM2IHQ=s64","userId":"11783081476684672772"}},"outputId":"e676b2d6-cfd4-4eb8-ba2e-d9ee1e5d1ae8"},"source":["def print_out(q, k, v):\n","    temp_out, temp_attn = scaled_dot_product_attention(q, k, v, None)\n","    print('Attention weights are:')\n","    print(np.round(temp_attn, decimals=2))\n","    print('Output is:')\n","    print(np.round(temp_out, decimals=2))\n","\n","temp_k = tf.constant([[10, 0, 0],\n","                      [0, 10, 0],\n","                      [0, 0, 10],\n","                      [0, 0, 10]], dtype=tf.float32)  # (4, 3)\n","\n","temp_v = tf.constant([[1, 0],\n","                      [10, 0],\n","                      [100, 5],\n","                      [1000, 6]], dtype=tf.float32)  # (4, 2)\n","\n","\n","# The dot product attention is selecting the key that aligns with the \n","# query and then returning the associated value.\n","temp_q = tf.constant([[0, 10, 0]], dtype=tf.float32)  # (1, 3)\n","print_out(temp_q, temp_k, temp_v)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Attention weights are:\n","[[0. 1. 0. 0.]]\n","Output is:\n","[[10.  0.]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"B1fsCrjh02cu"},"source":["Use the `scaled_dot_product_attention` layer to get a handle on how the model is selecting query-key pairs and computing their value.\n"]},{"cell_type":"code","metadata":{"id":"fcjJuOdxAHt7"},"source":["# sec 3.2.2\n","class MultiHeadAttention(tf.keras.layers.Layer):\n","    def __init__(self, d_model, num_heads):\n","        super(MultiHeadAttention, self).__init__()\n","        self.num_heads = num_heads\n","        self.d_model = d_model\n","\n","        assert self.d_model % self.num_heads == 0 \n","\n","        self.depth = self.d_model  // num_heads\n","\n","        self.wq = layers.Dense(d_model)\n","        self.wk = layers.Dense(d_model)\n","        self.wv = layers.Dense(d_model)\n","\n","        self.dense = layers.Dense(d_model)\n","\n","    def split_heads(self, x, batch_size):\n","        \"\"\"\n","        The inputs need to be reshaped in order to be fed into the attention portion.\n","        The model dimension d_k get split into heads x depth.\n","        Then transposed to (batch_size, num_heads, seq_len, depth)\n","\n","        \"\"\"\n","        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n","        return tf.transpose(x, perm=[0, 2, 1, 3])\n","\n","    # forward computation\n","    def call(self, v, k, q, mask):\n","        batch_size = tf.shape(q)[0]\n","\n","        q = self.wq(q) # (batch_size, seq_len, d_model)\n","        k = self.wk(k)\n","        v = self.wv(v)\n","\n","        q = self.split_heads(q, batch_size) # (batch_size, num_heads, seq_len_q, depth)\n","        k = self.split_heads(k, batch_size)\n","        v = self.split_heads(v, batch_size)\n","\n","        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n","        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n","        scaled_attention, attention_weights = scaled_dot_product_attention(q, k, v, mask)\n","\n","        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n","\n","        concat_attention = tf.reshape(scaled_attention,\n","                                    (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n","\n","        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n","\n","        return output, attention_weights\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1ZXWw8OQrT2T","executionInfo":{"status":"ok","timestamp":1624123270579,"user_tz":360,"elapsed":61,"user":{"displayName":"Tim Sullivan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgDOmSXGsobJcA9UOtHRwU-sWjSZffSAqqvuM2IHQ=s64","userId":"11783081476684672772"}},"outputId":"806beebd-073b-493c-cb10-f502967fa32c"},"source":["tf.random.set_seed(42)\n","mha = MultiHeadAttention(512, 8)\n","\n","x = tf.ones((1,60, 512))\n","out, attn = mha(x,k=x,q=x, mask=None)\n","out.shape, attn.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(TensorShape([1, 60, 512]), TensorShape([1, 8, 60, 60]))"]},"metadata":{"tags":[]},"execution_count":23}]},{"cell_type":"markdown","metadata":{"id":"txzzj49L_30r"},"source":["# Data\n","- Used English-German and English-French dataset. 4.5 million and 36 million sentences respectively\n","- 37000 word English-German vocab\n","- 32000 word English-French vocab\n","- Instead let's use the TF dataset for Russian to English"]},{"cell_type":"code","metadata":{"id":"xcFqTEr83YFW"},"source":["examples, metadata = tfds.load('ted_hrlr_translate/ru_to_en', with_info=True,\n","                               as_supervised=True)\n","train_examples, val_examples = examples['train'], examples['validation']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zkFVV1l956It","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1624123270915,"user_tz":360,"elapsed":19,"user":{"displayName":"Tim Sullivan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgDOmSXGsobJcA9UOtHRwU-sWjSZffSAqqvuM2IHQ=s64","userId":"11783081476684672772"}},"outputId":"9c3251d9-7515-4921-f32b-acf0fd8f3ec7"},"source":["#print  examples\n","for ru, en in train_examples.take(1):\n","  print(\"Russian: \", ru.numpy().decode('utf-8'))\n","  print(\"English:   \", en.numpy().decode('utf-8'))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Russian:  к : успех , перемены возможны только с оружием в руках .\n","English:    c : success , the change is only coming through the barrel of the gun .\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"7M6umRAu6e9t"},"source":["### Tokenization\n","- There's no tokenizer for the `ted hrlr` Russian to English set that I can find so I have to make one.\n","- The Google docs example for attention said they used a sub-word version built with Bert.\n","- I'll just use the example here https://www.tensorflow.org/text/guide/subwords_tokenizer"]},{"cell_type":"code","metadata":{"id":"TioATfFL8tdX"},"source":["!pip install -q -U tensorflow-text"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WFL54Yfg614k"},"source":["\n","import tensorflow_text as text\n","from tensorflow_text.tools.wordpiece_vocab import bert_vocab_from_dataset as bert_vocab\n","\n","train_en = train_examples.map(lambda ru, en: en)\n","train_ru = train_examples.map(lambda ru, en: ru)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Fjtfi7DkPM09"},"source":["This section will generate the English and Russain vocabulary, but it takes a long time so use the included `txt` files."]},{"cell_type":"code","metadata":{"id":"wFxzKMJi8xbD"},"source":["bert_tokenizer_params=dict(lower_case=True)\n","reserved_tokens=[\"[PAD]\", \"[UNK]\", \"[START]\", \"[END]\"]\n","\n","\n","# bert_vocab_args = dict(\n","#     # The target vocabulary size\n","#     vocab_size = 8000,\n","#     # Reserved tokens that must be included in the vocabulary\n","#     reserved_tokens=reserved_tokens,\n","#     # Arguments for `text.BertTokenizer`\n","#     bert_tokenizer_params=bert_tokenizer_params,\n","#     # Arguments for `wordpiece_vocab.wordpiece_tokenizer_learner_lib.learn`\n","#     learn_params={},\n","# )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"plzF-nFw9Bps"},"source":["# %%time\n","# en_vocab = bert_vocab.bert_vocab_from_dataset(\n","#     train_en.batch(1000).prefetch(2),\n","#     **bert_vocab_args\n","# )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"t2Hkjven9CSD"},"source":["# print(en_vocab[:10])\n","# print(en_vocab[100:110])\n","# print(en_vocab[1000:1010])\n","# print(en_vocab[-10:])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QDzCJsx-9CeJ"},"source":["# %%time\n","# ru_vocab = bert_vocab.bert_vocab_from_dataset(\n","#     train_ru.batch(1000).prefetch(2),\n","#     **bert_vocab_args\n","# )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NobcBT1P9CmQ"},"source":["# print(ru_vocab[:10])\n","# print(ru_vocab[100:110])\n","# print(ru_vocab[1000:1010])\n","# print(ru_vocab[-10:])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jSajpI4T9CuF"},"source":["# def write_vocab_file(filepath, vocab):\n","#   with open(filepath, 'w') as f:\n","#     for token in vocab:\n","#       print(token, file=f)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gYVqjS_b9C1J"},"source":["# write_vocab_file('en_vocab.txt', en_vocab)\n","# write_vocab_file('ru_vocab.txt', ru_vocab)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"n0pWFxlt9DDS"},"source":["# copying this module from the TF docs.\n","\n","import pathlib\n","import re\n","\n","START = tf.argmax(tf.constant(reserved_tokens) == \"[START]\")\n","END = tf.argmax(tf.constant(reserved_tokens) == \"[END]\")\n","\n","def cleanup_text(reserved_tokens, token_txt):\n","  # Drop the reserved tokens, except for \"[UNK]\".\n","  bad_tokens = [re.escape(tok) for tok in reserved_tokens if tok != \"[UNK]\"]\n","  bad_token_re = \"|\".join(bad_tokens)\n","\n","  bad_cells = tf.strings.regex_full_match(token_txt, bad_token_re)\n","  result = tf.ragged.boolean_mask(token_txt, ~bad_cells)\n","\n","  # Join them into strings.\n","  result = tf.strings.reduce_join(result, separator=' ', axis=-1)\n","\n","  return result\n","\n","def add_start_end(ragged):\n","  count = ragged.bounding_shape()[0]\n","  starts = tf.fill([count,1], START)\n","  ends = tf.fill([count,1], END)\n","  return tf.concat([starts, ragged, ends], axis=1)\n","\n","class CustomTokenizer(tf.Module):\n","  def __init__(self, reserved_tokens, vocab_path):\n","    self.tokenizer = text.BertTokenizer(vocab_path, lower_case=True)\n","    self._reserved_tokens = reserved_tokens\n","    self._vocab_path = tf.saved_model.Asset(vocab_path)\n","\n","    vocab = pathlib.Path(vocab_path).read_text().splitlines()\n","    self.vocab = tf.Variable(vocab)\n","\n","    ## Create the signatures for export:   \n","\n","    # Include a tokenize signature for a batch of strings. \n","    self.tokenize.get_concrete_function(\n","        tf.TensorSpec(shape=[None], dtype=tf.string))\n","\n","    # Include `detokenize` and `lookup` signatures for:\n","    #   * `Tensors` with shapes [tokens] and [batch, tokens]\n","    #   * `RaggedTensors` with shape [batch, tokens]\n","    self.detokenize.get_concrete_function(\n","        tf.TensorSpec(shape=[None, None], dtype=tf.int64))\n","    self.detokenize.get_concrete_function(\n","          tf.RaggedTensorSpec(shape=[None, None], dtype=tf.int64))\n","\n","    self.lookup.get_concrete_function(\n","        tf.TensorSpec(shape=[None, None], dtype=tf.int64))\n","    self.lookup.get_concrete_function(\n","          tf.RaggedTensorSpec(shape=[None, None], dtype=tf.int64))\n","\n","    # These `get_*` methods take no arguments\n","    self.get_vocab_size.get_concrete_function()\n","    self.get_vocab_path.get_concrete_function()\n","    self.get_reserved_tokens.get_concrete_function()\n","\n","  @tf.function\n","  def tokenize(self, strings):\n","    enc = self.tokenizer.tokenize(strings)\n","    # Merge the `word` and `word-piece` axes.\n","    enc = enc.merge_dims(-2,-1)\n","    enc = add_start_end(enc)\n","    return enc\n","\n","  @tf.function\n","  def detokenize(self, tokenized):\n","    words = self.tokenizer.detokenize(tokenized)\n","    return cleanup_text(self._reserved_tokens, words)\n","\n","  @tf.function\n","  def lookup(self, token_ids):\n","    return tf.gather(self.vocab, token_ids)\n","\n","  @tf.function\n","  def get_vocab_size(self):\n","    return tf.shape(self.vocab)[0]\n","\n","  @tf.function\n","  def get_vocab_path(self):\n","    return self._vocab_path\n","\n","  @tf.function\n","  def get_reserved_tokens(self):\n","    return tf.constant(self._reserved_tokens)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GubNU_HK_dX7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1624123276943,"user_tz":360,"elapsed":3271,"user":{"displayName":"Tim Sullivan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgDOmSXGsobJcA9UOtHRwU-sWjSZffSAqqvuM2IHQ=s64","userId":"11783081476684672772"}},"outputId":"5440e25f-fba1-4d6a-8e89-34f6f2909705"},"source":["tokenizers = tf.Module()\n","tokenizers.ru = CustomTokenizer(reserved_tokens, 'ru_vocab.txt')\n","tokenizers.en = CustomTokenizer(reserved_tokens, 'en_vocab.txt')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/array_ops.py:5049: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n","Instructions for updating:\n","The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/array_ops.py:5049: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n","Instructions for updating:\n","The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"Q_bxYzmM_il8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1624123278894,"user_tz":360,"elapsed":1959,"user":{"displayName":"Tim Sullivan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgDOmSXGsobJcA9UOtHRwU-sWjSZffSAqqvuM2IHQ=s64","userId":"11783081476684672772"}},"outputId":"3138d17d-4572-4901-8621-d15b14b1feca"},"source":["model_name = 'ted_hrlr_translate_ru_en_converter'\n","tf.saved_model.save(tokenizers, model_name);"],"execution_count":null,"outputs":[{"output_type":"stream","text":["INFO:tensorflow:Assets written to: ted_hrlr_translate_ru_en_converter/assets\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Assets written to: ted_hrlr_translate_ru_en_converter/assets\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"cmLu95UP_oLK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1624123280146,"user_tz":360,"elapsed":1256,"user":{"displayName":"Tim Sullivan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgDOmSXGsobJcA9UOtHRwU-sWjSZffSAqqvuM2IHQ=s64","userId":"11783081476684672772"}},"outputId":"7bfb0cb5-1199-4609-a175-99c0480a9b72"},"source":["reloaded_tokenizers = tf.saved_model.load(model_name)\n","reloaded_tokenizers.en.get_vocab_size().numpy()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["7796"]},"metadata":{"tags":[]},"execution_count":38}]},{"cell_type":"code","metadata":{"id":"AN7GpSbX_rhe","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1624123280931,"user_tz":360,"elapsed":789,"user":{"displayName":"Tim Sullivan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgDOmSXGsobJcA9UOtHRwU-sWjSZffSAqqvuM2IHQ=s64","userId":"11783081476684672772"}},"outputId":"b2627f89-dd28-4788-f83a-ac1448e8d831"},"source":["tokens = reloaded_tokenizers.en.tokenize(['Hello TensorFlow!'])\n","print(tokens.numpy())\n","\n","round_trip = reloaded_tokenizers.en.detokenize(tokens)\n","print(round_trip.numpy()[0].decode('utf-8'))\n","\n","print(tokenizers.en.lookup(tokens))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[[   2 3372 2214  691  952 2669    4    3]]\n","hello tensorflow !\n","<tf.RaggedTensor [[b'[START]', b'hello', b'tens', b'##or', b'##f', b'##low', b'!', b'[END]']]>\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"icVKXWT4ymXS"},"source":["Make the pipeline"]},{"cell_type":"code","metadata":{"id":"Dm7Eby5PyjWZ"},"source":["def tokenize_pairs(ru, en):\n","    ru = tokenizers.ru.tokenize(ru)\n","    # Convert from ragged to dense, padding with zeros.\n","    ru = ru.to_tensor()\n","\n","    en = tokenizers.en.tokenize(en)\n","    # Convert from ragged to dense, padding with zeros.\n","    en = en.to_tensor()\n","    return ru, en"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3PzlFybPyrRD"},"source":["BUFFER_SIZE = 20000\n","BATCH_SIZE = 32 # reduced for memory"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"40Y0Y2qOyrzw"},"source":["def make_batches(ds):\n","  return (\n","      ds\n","      .cache()\n","      .shuffle(BUFFER_SIZE)\n","      .batch(BATCH_SIZE)\n","      .map(tokenize_pairs, num_parallel_calls=tf.data.AUTOTUNE)\n","      .prefetch(tf.data.AUTOTUNE))\n","\n","\n","train_batches = make_batches(train_examples)\n","val_batches = make_batches(val_examples)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XRZUv6RPzklZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1624123283112,"user_tz":360,"elapsed":1508,"user":{"displayName":"Tim Sullivan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgDOmSXGsobJcA9UOtHRwU-sWjSZffSAqqvuM2IHQ=s64","userId":"11783081476684672772"}},"outputId":"dbfbdebb-5022-4da7-8c0e-ff108fb7315f"},"source":["list(train_batches.take(2).as_numpy_iterator())"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[(array([[   2,   70, 3322, ...,    0,    0,    0],\n","         [   2, 1017,  148, ...,    0,    0,    0],\n","         [   2,  107,  181, ...,    0,    0,    0],\n","         ...,\n","         [   2,  469,   88, ...,    0,    0,    0],\n","         [   2,   79,  368, ...,    0,    0,    0],\n","         [   2,   10,  678, ...,    0,    0,    0]]),\n","  array([[   2,   38,   38, ...,    0,    0,    0],\n","         [   2,   85,  108, ...,    0,    0,    0],\n","         [   2,   47, 6750, ...,    0,    0,    0],\n","         ...,\n","         [   2,  102,   84, ...,    0,    0,    0],\n","         [   2,   85,  108, ...,    0,    0,    0],\n","         [   2,   10,  365, ...,    0,    0,    0]])),\n"," (array([[   2,   86, 2857, ...,    0,    0,    0],\n","         [   2,  163,   14, ...,    0,    0,    0],\n","         [   2,  333,  131, ...,  548,   16,    3],\n","         ...,\n","         [   2,   40,   40, ...,    0,    0,    0],\n","         [   2,  471,  231, ...,    0,    0,    0],\n","         [   2,  435,   14, ...,    0,    0,    0]]),\n","  array([[   2, 3164,   89, ...,    0,    0,    0],\n","         [   2,   89,   94, ...,    0,    0,    0],\n","         [   2,   90,    9, ..., 1061,   16,    3],\n","         ...,\n","         [   2,   38,   38, ...,    0,    0,    0],\n","         [   2,  108,   99, ...,    0,    0,    0],\n","         [   2,   92,  138, ...,    0,    0,    0]]))]"]},"metadata":{"tags":[]},"execution_count":43}]},{"cell_type":"markdown","metadata":{"id":"GQlcI-Y91k3P"},"source":["## Position encoding\n","\n","We ditched the recurrent structure due to the long gradient problem, but the position of words in the sequence is still valuable.\n","\n","The embedding groups words by meaning, and adding the positional encoding adds absolute and relative position information.\n","\n","$$ PE_{(pos, 2i)}=sin(pos/10000^{2i/d_{model}})$$\n","$$ PE_{(pos, 2i+1)}=cos(pos/10000^{2i/d_{model}})$$"]},{"cell_type":"code","metadata":{"id":"QMNqzpIF1NBx"},"source":["def get_angles(pos, i , d_model):\n","    angle_rates = 1 / np.power(10000, (2*(i//2)) / np.float32(d_model))\n","    return pos * angle_rates\n","\n","def positional_encoding(position, d_model):\n","    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n","                            np.arange(d_model)[np.newaxis,:],\n","                            d_model)\n","    # sin to even i\n","    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n","\n","    # cos to odd indices 2i+1\n","    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n","\n","    pos_encoding = angle_rads[np.newaxis, ...]\n","\n","    return tf.cast(pos_encoding, dtype=tf.float32)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":463},"id":"84r-xeHW3IH_","executionInfo":{"status":"ok","timestamp":1624126543012,"user_tz":360,"elapsed":17,"user":{"displayName":"Tim Sullivan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgDOmSXGsobJcA9UOtHRwU-sWjSZffSAqqvuM2IHQ=s64","userId":"11783081476684672772"}},"outputId":"1e2bdf5b-22ba-44b8-935e-c2298d71f84a"},"source":["n, d = 25, 512\n","pos_encoding = positional_encoding(n, d)\n","print(pos_encoding.shape)\n","pos_encoding = pos_encoding[0]\n","\n","# Juggle the dimensions for the plot\n","pos_encoding = tf.reshape(pos_encoding, (n, d//2, 2))\n","pos_encoding = tf.transpose(pos_encoding, (2, 1, 0))\n","pos_encoding = tf.reshape(pos_encoding, (d, n))\n","\n","import matplotlib.pyplot as plt\n","plt.figure(figsize=(10,7))\n","plt.pcolormesh(pos_encoding[:20,:], cmap='RdBu')\n","plt.ylabel('Depth')\n","plt.xlabel('Position')\n","plt.colorbar()\n","plt.show()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(1, 25, 512)\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 720x504 with 2 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"PPn5VkRy2ZOv"},"source":["## Masking\n","\n","Two types of masking are required. One is for the tokenized inputs which are padding with zeros. This is the padding mask. The other is to mask other tokens in the sequence that the model is trying to learn. This is the look ahead mask."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iVOkOKn178UL","executionInfo":{"status":"ok","timestamp":1623885428727,"user_tz":360,"elapsed":39,"user":{"displayName":"Tim Sullivan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgDOmSXGsobJcA9UOtHRwU-sWjSZffSAqqvuM2IHQ=s64","userId":"11783081476684672772"}},"outputId":"6813c207-4bbf-44f4-c223-ce731706ccfc"},"source":["def create_padding_mask(seq):\n","    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n","\n","    # adding dimensions for attention logits compabitility\n","    return seq[:, tf.newaxis, tf.newaxis, :] #(batch_size, 1, 1, seq_len)\n","\n","x = tf.constant([[7, 6, 0, 0, 1], [1, 2, 3, 0, 0], [0, 0, 0, 4, 5]])\n","create_padding_mask(x)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(3, 1, 1, 5), dtype=float32, numpy=\n","array([[[[0., 0., 1., 1., 0.]]],\n","\n","\n","       [[[0., 0., 0., 1., 1.]]],\n","\n","\n","       [[[1., 1., 1., 0., 0.]]]], dtype=float32)>"]},"metadata":{"tags":[]},"execution_count":30}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UP5HctVp9Um3","executionInfo":{"status":"ok","timestamp":1623885428728,"user_tz":360,"elapsed":30,"user":{"displayName":"Tim Sullivan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgDOmSXGsobJcA9UOtHRwU-sWjSZffSAqqvuM2IHQ=s64","userId":"11783081476684672772"}},"outputId":"e1e59428-4211-41d4-f88b-7a11287ce1ef"},"source":["def create_look_ahead_mask(size):\n","    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n","    return mask  # (seq_len, seq_len)\n","\n","x = tf.random.uniform((1, 3))\n","temp = create_look_ahead_mask(x.shape[1])\n","temp"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(3, 3), dtype=float32, numpy=\n","array([[0., 1., 1.],\n","       [0., 0., 1.],\n","       [0., 0., 0.]], dtype=float32)>"]},"metadata":{"tags":[]},"execution_count":31}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8LhbGTM4-SA1","executionInfo":{"status":"ok","timestamp":1623885428729,"user_tz":360,"elapsed":23,"user":{"displayName":"Tim Sullivan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgDOmSXGsobJcA9UOtHRwU-sWjSZffSAqqvuM2IHQ=s64","userId":"11783081476684672772"}},"outputId":"f4a27ed0-e33a-4732-fb16-66d798026e04"},"source":["a = tf.ones((3,3))\n","tf.linalg.band_part(a, 0, 0)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(3, 3), dtype=float32, numpy=\n","array([[1., 0., 0.],\n","       [0., 1., 0.],\n","       [0., 0., 1.]], dtype=float32)>"]},"metadata":{"tags":[]},"execution_count":32}]},{"cell_type":"markdown","metadata":{"id":"SmfNEG-_1j0r"},"source":["## Embedding\n","- Learned embeddings convert tokens to vectors of dimension $d_\\text{model}$"]},{"cell_type":"markdown","metadata":{"id":"EwkWYTQ1A0jg"},"source":["# Model"]},{"cell_type":"markdown","metadata":{"id":"BRRLenCxtfMw"},"source":["## Encoder\n","- 6 layers\n","- 2 sub-layers per layer\n","    - Embedd\n","    - Position encoding\n","    - Multi-head self attention\n","    - FC Feed-forward\n","    - Residual connections around each sub-layer\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"LJBow0-_1RiP"},"source":["# The encoder and decoder also contain a feed forward section with two dense\n","# layers and relu activation between\n","def point_wise_feed_forward_network(d_model, d_ff):\n","    return tf.keras.Sequential([layers.Dense(d_ff, activation='relu'),\n","                                layers.Dense(d_model)])\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_QfqTCVrj9FX"},"source":["class EncoderLayer(layers.Layer):\n","    def __init__(self, d_model, num_heads, d_ff, dropout_rate=0.1):\n","        super(EncoderLayer, self).__init__()\n","\n","        self.mha = MultiHeadAttention(d_model, num_heads)\n","        self.ffn = point_wise_feed_forward_network(d_model, d_ff)\n","\n","        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n","        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n","\n","        self.dropout1 = layers.Dropout(dropout_rate)\n","        self.dropout2 = layers.Dropout(dropout_rate)\n","\n","    def call(self, x, training, mask):\n","        attn_out, _ = self.mha(x, x, x, mask) # (batch_size, input_seq_len, d_model)\n","        attn_out = self.dropout1(attn_out, training=training)\n","        out1 = self.layernorm1(x + attn_out)\n","\n","        ff_out = self.ffn(out1)\n","        ff_out = self.dropout2(ff_out, training=training)\n","        out2 = self.layernorm2(out1 + ff_out) # (batch_size, input_seq_len, d_model)\n","\n","        return out2\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DgJvMyUtLAdu","executionInfo":{"status":"ok","timestamp":1623885428889,"user_tz":360,"elapsed":11,"user":{"displayName":"Tim Sullivan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgDOmSXGsobJcA9UOtHRwU-sWjSZffSAqqvuM2IHQ=s64","userId":"11783081476684672772"}},"outputId":"d86c322f-4159-4e58-faad-f73ed0ea105e"},"source":["tf.random.set_seed(42)\n","sample_encoder_layer = EncoderLayer(512, 8, 2048)\n","\n","sample_encoder_layer_output = sample_encoder_layer(\n","    tf.random.uniform((64, 43, 512)), False, None)\n","\n","sample_encoder_layer_output.shape  # (batch_size, input_seq_len, d_model)\n"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["TensorShape([64, 43, 512])"]},"metadata":{"tags":[]},"execution_count":35}]},{"cell_type":"code","metadata":{"id":"vDuyHtLXj9AH"},"source":["class Encoder(layers.Layer):\n","    def __init__(self, num_layers, d_model, num_heads, d_ff, input_vocab_size,\n","                 maximum_position_encoding, dropout_rate=0.1):\n","        super(Encoder, self).__init__()\n","\n","        self.d_model = d_model\n","        self.num_layers = num_layers\n","\n","        self.embedding = layers.Embedding(input_vocab_size, d_model)\n","        self.pos_encoding = positional_encoding(maximum_position_encoding,\n","                                                self.d_model)\n","        \n","        self.enc_layers = [EncoderLayer(d_model, num_heads, d_ff, dropout_rate)\n","                            for i in range(num_layers)]\n","        self.dropout = layers.Dropout(dropout_rate)\n","\n","    def call(self, x, training, mask):\n","        \n","        seq_len = tf.shape(x)[1]\n","        \n","        # prep inputs with embedding and pos encoding\n","        x = self.embedding(x) # (batch_size, input_seq_len, d_model)\n","        # embedding layer weights are multiplied by sqrt(d_model) sec 3.4\n","        x *= tf.sqrt(tf.cast(self.d_model, tf.float32))\n","        x += self.pos_encoding[:, :seq_len, :]\n","        \n","\n","        x = self.dropout(x, training=training)\n","        # now go through all the encoding layers\n","        for i in range(self.num_layers):\n","            x = self.enc_layers[i](x, training, mask)\n","\n","        return x # (batch_size, input_seq_len, d_model)\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7P_7olzrj85P","executionInfo":{"status":"ok","timestamp":1623885429033,"user_tz":360,"elapsed":152,"user":{"displayName":"Tim Sullivan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgDOmSXGsobJcA9UOtHRwU-sWjSZffSAqqvuM2IHQ=s64","userId":"11783081476684672772"}},"outputId":"8814891d-7df9-4b76-e1ac-c689be49ef1d"},"source":["tf.random.set_seed(42)\n","sample_encoder = Encoder(num_layers=2, d_model=512, num_heads=8,\n","                         d_ff=2048, input_vocab_size=8500,\n","                         maximum_position_encoding=10000)\n","temp_input = tf.random.uniform((64, 62), dtype=tf.int64, minval=0, maxval=200)\n","sample_encoder_output = sample_encoder(temp_input, training=False, mask=None)\n","\n","print(sample_encoder_output.shape)  # (batch_size, input_seq_len, d_model)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(64, 62, 512)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"-F8ZPl9w0g02"},"source":["## Decoder\n","- 6 layers\n","- Same 2 layers as Encoder but with a third layer between\n","    - Middle layer performs multi-head attention on output from Encoder\n","    - Same residual connections"]},{"cell_type":"code","metadata":{"id":"tuuFQ_GHmXGL"},"source":["class DecoderLayer(layers.Layer):\n","    def __init__(self, d_model, num_heads, d_ff, dropout_rate=0.1):\n","        super(DecoderLayer, self).__init__()\n","\n","        self.mmha = MultiHeadAttention(d_model, num_heads)\n","        self.mha = MultiHeadAttention(d_model, num_heads)\n","        self.ffn = point_wise_feed_forward_network(d_model, d_ff)\n","\n","        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n","        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n","        self.layernorm3 = layers.LayerNormalization(epsilon=1e-6)\n","\n","        self.dropout1 = layers.Dropout(dropout_rate)\n","        self.dropout2 = layers.Dropout(dropout_rate)\n","        self.dropout3 = layers.Dropout(dropout_rate)\n","\n","    def call(self, x, encoder_out, training, look_ahead_mask, padding_mask):\n","        mask_attn_out, attn_weights_block1 = self.mmha(x, x, x, look_ahead_mask) # (batch_size, target_seq_len, d_model)\n","        mask_attn_out = self.dropout1(mask_attn_out, training=training)\n","        out1 = self.layernorm1(mask_attn_out + x)\n","\n","        attn_out, attn_weights_block2 = self.mha(encoder_out, encoder_out, \n","                                                 out1, padding_mask)\n","        attn_out = self.dropout2(attn_out, training=training)\n","        out2 = self.layernorm2(attn_out + out1)\n","\n","        ff_out = self.ffn(out2)\n","        ff_out = self.dropout3(ff_out, training=training)\n","        out3 = self.layernorm3(ff_out + out2) # (batch_size, target_seq_len, d_model)\n","\n","        return out3, attn_weights_block1, attn_weights_block2"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"F4dTu2D5mXNJ"},"source":["class Decoder(layers.Layer):\n","    def __init__(self, num_layers, d_model, num_heads, d_ff, target_vocab_size,\n","                 maximum_position_encoding, dropout_rate=0.1):\n","        super(Decoder, self).__init__()\n","\n","        self.d_model = d_model\n","        self.num_layers = num_layers\n","\n","        self.embedding = layers.Embedding(target_vocab_size, d_model)\n","        self.pos_encoding = positional_encoding(maximum_position_encoding,\n","                                                d_model)\n","        \n","        self.dec_layers = [DecoderLayer(d_model, num_heads, d_ff, dropout_rate)\n","                            for i in range(num_layers)]\n","        self.dropout = layers.Dropout(dropout_rate)\n","\n","    def call(self, x, enc_output, training,  \n","             look_ahead_mask, padding_mask):\n","        \n","        seq_len = tf.shape(x)[1]\n","        attn_weights = {}\n","        \n","        # prep inputs with embedding and pos encoding\n","        x = self.embedding(x) # (batch_size, input_seq_len, d_model)\n","        # embedding layer weights are multiplied by sqrt(d_model) sec 3.4\n","        x *= tf.sqrt(tf.cast(self.d_model, tf.float32))\n","        x += self.pos_encoding[:, :seq_len, :]\n","\n","        x = self.dropout(x, training=training)\n","\n","        # now go through all the encoding layers\n","        for i in range(self.num_layers):\n","            x, block1, block2 = self.dec_layers[i](x, enc_output, training, \n","                                                   look_ahead_mask,padding_mask)\n","            attn_weights[f'decoder_layer{i+1}_block1'] = block1\n","            attn_weights[f'decoder_layer{i+1}_block2'] = block2\n","\n","        return x, attn_weights # (batch_size, input_seq_len, d_model)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yMbNCs5fmXUJ","executionInfo":{"status":"ok","timestamp":1623885429218,"user_tz":360,"elapsed":192,"user":{"displayName":"Tim Sullivan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgDOmSXGsobJcA9UOtHRwU-sWjSZffSAqqvuM2IHQ=s64","userId":"11783081476684672772"}},"outputId":"189ea5c3-123e-4a90-cfcd-66cb825345fc"},"source":["tf.random.set_seed(42)\n","sample_decoder = Decoder(num_layers=2, d_model=512, num_heads=8,\n","                         d_ff=2048, target_vocab_size=8000,\n","                         maximum_position_encoding=5000)\n","temp_input = tf.random.uniform((64, 26), dtype=tf.int64, minval=0, maxval=200)\n","\n","output, attn = sample_decoder(temp_input,\n","                              enc_output=sample_encoder_output,\n","                              training=False,\n","                              look_ahead_mask=None,\n","                              padding_mask=None)\n","\n","output.shape, attn['decoder_layer2_block2'].shape\n"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(TensorShape([64, 26, 512]), TensorShape([64, 8, 26, 62]))"]},"metadata":{"tags":[]},"execution_count":40}]},{"cell_type":"markdown","metadata":{"id":"ZsDVHDjNmXiN"},"source":["# Transformer\n","Finally construct the full transformer."]},{"cell_type":"code","metadata":{"id":"36gPZPqGeCFH"},"source":["class Transformer(tf.keras.Model):\n","    def __init__(self, num_layers, d_model, num_heads, d_ff, input_vocab_size,\n","                 target_vocab_size, pe_input, pe_target, dropout_rate=0.1):\n","        super(Transformer, self).__init__()\n","    \n","        self.encoder = Encoder(num_layers, d_model, num_heads, d_ff, \n","                            input_vocab_size, pe_input, dropout_rate)\n","        \n","        self.decoder = Decoder(num_layers, d_model, num_heads, d_ff, \n","                            input_vocab_size, pe_target, dropout_rate)\n","        \n","        self.final_layer = layers.Dense(target_vocab_size)\n","\n","    def call(self, inp, tar, training, enc_padding_mask, look_ahead_mask,\n","             dec_padding_mask):\n","        # (batch_size, inp_seq_len, d_model)\n","        enc_output = self.encoder(inp, training, enc_padding_mask)\n","\n","        dec_output, attention_weights  = self.decoder(tar, enc_output, training, \n","                                                look_ahead_mask,dec_padding_mask)\n","        \n","        # (batch_size, tar_seq_len, target_vocab_size)\n","        final_output = self.final_layer(dec_output)\n","\n","        return final_output, attention_weights\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_vXPBqt8eCVZ","executionInfo":{"status":"ok","timestamp":1623885429873,"user_tz":360,"elapsed":529,"user":{"displayName":"Tim Sullivan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgDOmSXGsobJcA9UOtHRwU-sWjSZffSAqqvuM2IHQ=s64","userId":"11783081476684672772"}},"outputId":"c4473199-46d9-4868-d7c5-ee02681e1594"},"source":["tf.random.set_seed(42)\n","sample_transformer = Transformer(\n","    num_layers=2, d_model=512, num_heads=8, d_ff=2048,\n","    input_vocab_size=8500, target_vocab_size=8000,\n","    pe_input=10000, pe_target=6000)\n","\n","temp_input = tf.random.uniform((64, 38), dtype=tf.int64, minval=0, maxval=200)\n","temp_target = tf.random.uniform((64, 36), dtype=tf.int64, minval=0, maxval=200)\n","\n","fn_out, _ = sample_transformer(temp_input, temp_target, training=False,\n","                               enc_padding_mask=None,\n","                               look_ahead_mask=None,\n","                               dec_padding_mask=None)\n","\n","fn_out.shape  # (batch_size, tar_seq_len, target_vocab_size)\n"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["TensorShape([64, 36, 8000])"]},"metadata":{"tags":[]},"execution_count":42}]},{"cell_type":"markdown","metadata":{"id":"Pj-VpGSNADa9"},"source":["# Train\n","\n"]},{"cell_type":"markdown","metadata":{"id":"OYmgpx8_eCj8"},"source":["Hyperparameters"]},{"cell_type":"code","metadata":{"id":"9iFP1Ebef5gc"},"source":["num_layers = 4\n","d_model = 128\n","d_ff = 512\n","num_heads = 8\n","dropout_rate = 0.1"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-U9XOspyAF1s"},"source":["## Optimizer\n","- Adam $\\beta_1$ = 0.9, $\\beta_2$ = 0.98 and $\\epsilon$ = $10^{-9}$\n","- Custom learning rate scheduler\n","    - increase learning rate linearly for `warm_up_steps` then decrease it proportionally according to the step number."]},{"cell_type":"code","metadata":{"id":"XMNOUryoeCrs"},"source":["# learning rate scheduler, sec 5.3\n","class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n","    def __init__(self, d_model, warmup_steps = 4000):\n","        super(CustomSchedule, self).__init__()\n","        \n","        self.d_model = tf.cast(d_model, dtype=tf.float32)\n","        self.warmup_steps = warmup_steps\n","\n","    def __call__(self, step):\n","        lr = tf.math.rsqrt(self.d_model) * tf.math.minimum(tf.math.rsqrt(step), \n","                                                           step*self.warmup_steps**-1.5)\n","\n","        return lr\n","learning_rate = CustomSchedule(d_model)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":296},"id":"lurX5yLAW9aJ","executionInfo":{"status":"ok","timestamp":1623885430027,"user_tz":360,"elapsed":160,"user":{"displayName":"Tim Sullivan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgDOmSXGsobJcA9UOtHRwU-sWjSZffSAqqvuM2IHQ=s64","userId":"11783081476684672772"}},"outputId":"60537911-f5d9-405b-973b-430ca984f400"},"source":["temp_learning_rate_schedule = CustomSchedule(d_model)\n","\n","plt.plot(temp_learning_rate_schedule(tf.range(40000, dtype=tf.float32)))\n","plt.ylabel(\"Learning Rate\")\n","plt.xlabel(\"Train Step\")"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Text(0.5, 0, 'Train Step')"]},"metadata":{"tags":[]},"execution_count":45},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"id":"0Hl_BmT-hP51"},"source":["optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98,\n","                                     epsilon=1e-9)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_OQBEWzusNUG"},"source":["## Loss\n","The target sequences are padded so a mask needs to be applied to accurately calculate the loss.\n"]},{"cell_type":"code","metadata":{"id":"CLAGjViusrWy"},"source":["loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n","    from_logits=True, reduction='none')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Z2x5harEteEE"},"source":["def loss_function(real, pred):\n","    mask = tf.math.logical_not(tf.math.equal(real, 0))\n","    loss_ = loss_object(real, pred)\n","\n","    mask = tf.cast(mask, dtype=loss_.dtype)\n","    loss_ *= mask\n","\n","    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n","\n","def accuracy_function(real, pred):\n","    accuracies = tf.equal(real, tf.argmax(pred, axis=2))\n","\n","    mask = tf.math.logical_not(tf.math.equal(real, 0))\n","    accuracies = tf.math.logical_and(mask, accuracies)\n","\n","    accuracies = tf.cast(accuracies, dtype=tf.float32)\n","    mask = tf.cast(mask, dtype=tf.float32)\n","    return tf.reduce_sum(accuracies)/tf.reduce_sum(mask)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"njZBWdyNuZVy"},"source":["train_loss = tf.keras.metrics.Mean(name='train_loss')\n","train_accuracy = tf.keras.metrics.Mean(name='train_accuracy')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"becb-Io_ujkQ"},"source":["## Training and checkpoints\n","The targets for each input are the inputs shifted by one. The true output is always passed to the model at the next timestep regardless of what the model predicted at the current timestep. So the model at timestep $t$ is not hindered by the model's performance at $t-1$."]},{"cell_type":"code","metadata":{"id":"R3GQoY7PVcfr"},"source":["# examples, metadata = tfds.load('ted_hrlr_translate/pt_to_en', with_info=True,\n","#                                as_supervised=True)\n","# train_examples, val_examples = examples['train'], examples['validation']\n","\n","# model_name = \"ted_hrlr_translate_pt_en_converter\"\n","# tf.keras.utils.get_file(\n","#     f\"{model_name}.zip\",\n","#     f\"https://storage.googleapis.com/download.tensorflow.org/models/{model_name}.zip\",\n","#     cache_dir='.', cache_subdir='', extract=True\n","# )\n","\n","# tokenizers = tf.saved_model.load(model_name)\n","\n","# def tokenize_pairs(pt, en):\n","#     pt = tokenizers.pt.tokenize(pt)\n","#     # Convert from ragged to dense, padding with zeros.\n","#     pt = pt.to_tensor()\n","\n","#     en = tokenizers.en.tokenize(en)\n","#     # Convert from ragged to dense, padding with zeros.\n","#     en = en.to_tensor()\n","#     return pt, en\n","# def make_batches(ds):\n","#   return (\n","#       ds\n","#       .cache()\n","#     #   .shuffle(BUFFER_SIZE)\n","#       .batch(BATCH_SIZE)\n","#       .map(tokenize_pairs, num_parallel_calls=tf.data.AUTOTUNE)\n","#       .prefetch(tf.data.AUTOTUNE))\n","\n","# train_batches = make_batches(train_examples)\n","# val_batches = make_batches(val_examples)\n","\n","# tokenizers.ru = tokenizers.pt\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ufgL2_q7aYt-","executionInfo":{"status":"ok","timestamp":1623885430153,"user_tz":360,"elapsed":9,"user":{"displayName":"Tim Sullivan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgDOmSXGsobJcA9UOtHRwU-sWjSZffSAqqvuM2IHQ=s64","userId":"11783081476684672772"}},"outputId":"c8ecf7cb-e958-41a6-8aa9-03758d16b4b2"},"source":["for pt_examples, en_examples in train_examples.batch(3).take(1):\n","    for pt in pt_examples.numpy():\n","        print(pt.decode('utf-8'))\n","\n","    print()\n","\n","    for en in en_examples.numpy():\n","        print(en.decode('utf-8'))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["к : успех , перемены возможны только с оружием в руках .\n","документация и методика практического обучения также доступна и выпущена creative commons .\n","( видео ) диди пиклз : сейчас четыре часа утра .\n","\n","c : success , the change is only coming through the barrel of the gun .\n","the documentation and the hands-on teaching methodology is also open-source and released as the creative commons .\n","( video ) didi pickles : it 's four o'clock in the morning .\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Lo-SwvM6agyU","executionInfo":{"status":"ok","timestamp":1623885430731,"user_tz":360,"elapsed":585,"user":{"displayName":"Tim Sullivan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgDOmSXGsobJcA9UOtHRwU-sWjSZffSAqqvuM2IHQ=s64","userId":"11783081476684672772"}},"outputId":"997795fe-366c-4709-f7ac-aafa0c3c4fc7"},"source":["encoded = tokenizers.en.tokenize(en_examples)\n","\n","for row in encoded.to_list():\n","    print(row)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[2, 41, 28, 1103, 14, 84, 243, 93, 200, 389, 218, 84, 6405, 87, 84, 2473, 16, 3]\n","[2, 84, 3914, 464, 85, 84, 702, 15, 104, 1495, 2346, 2024, 93, 187, 435, 15, 942, 85, 2533, 111, 84, 1068, 5725, 16, 3]\n","[2, 10, 400, 11, 168, 379, 1026, 1125, 28, 90, 9, 57, 316, 53, 9, 2501, 89, 84, 813, 16, 3]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ddJk7htDalET","executionInfo":{"status":"ok","timestamp":1623885431070,"user_tz":360,"elapsed":343,"user":{"displayName":"Tim Sullivan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgDOmSXGsobJcA9UOtHRwU-sWjSZffSAqqvuM2IHQ=s64","userId":"11783081476684672772"}},"outputId":"4bc2fe8e-e3c2-45b7-823f-7f3422c50f76"},"source":["round_trip = tokenizers.en.detokenize(encoded)\n","for line in round_trip.numpy():\n","    print(line.decode('utf-8'))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["c : success , the change is only coming through the barrel of the gun .\n","the documentation and the hands - on teaching methodology is also open - source and released as the creative commons .\n","( video ) didi pickles : it ' s four o ' clock in the morning .\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bfGkqWV5ao5T","executionInfo":{"status":"ok","timestamp":1623885431071,"user_tz":360,"elapsed":12,"user":{"displayName":"Tim Sullivan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgDOmSXGsobJcA9UOtHRwU-sWjSZffSAqqvuM2IHQ=s64","userId":"11783081476684672772"}},"outputId":"08133cdf-4464-400f-c014-479c536354ce"},"source":["tokens = tokenizers.en.lookup(encoded)\n","tokens"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.RaggedTensor [[b'[START]', b'c', b':', b'success', b',', b'the', b'change', b'is', b'only', b'coming', b'through', b'the', b'barrel', b'of', b'the', b'gun', b'.', b'[END]'], [b'[START]', b'the', b'document', b'##ation', b'and', b'the', b'hands', b'-', b'on', b'teaching', b'method', b'##ology', b'is', b'also', b'open', b'-', b'source', b'and', b'released', b'as', b'the', b'creative', b'commons', b'.', b'[END]'], [b'[START]', b'(', b'video', b')', b'did', b'##i', b'pick', b'##les', b':', b'it', b\"'\", b's', b'four', b'o', b\"'\", b'clock', b'in', b'the', b'morning', b'.', b'[END]']]>"]},"metadata":{"tags":[]},"execution_count":54}]},{"cell_type":"code","metadata":{"id":"SnEY-bjiuluu","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1623885686874,"user_tz":360,"elapsed":306,"user":{"displayName":"Tim Sullivan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgDOmSXGsobJcA9UOtHRwU-sWjSZffSAqqvuM2IHQ=s64","userId":"11783081476684672772"}},"outputId":"8b717241-cb22-46df-d565-4b18e631f6f3"},"source":["# create the transformer\n","tf.random.set_seed(42)\n","\n","transformer = Transformer(num_layers = num_layers, \n","                          d_model = d_model,\n","                          num_heads = num_heads, \n","                          d_ff = d_ff,\n","                          input_vocab_size=tokenizers.ru.get_vocab_size(),\n","                          target_vocab_size = tokenizers.en.get_vocab_size(),\n","                          pe_input=1000,\n","                          pe_target=1000,\n","                          dropout_rate=dropout_rate)\n","\n","temp_input = tf.random.uniform((64, 38), dtype=tf.int64, minval=0, maxval=200)\n","temp_target = tf.random.uniform((64, 36), dtype=tf.int64, minval=0, maxval=200)\n","\n","fn_out, _ = sample_transformer(temp_input, temp_target, training=False,\n","                               enc_padding_mask=None,\n","                               look_ahead_mask=None,\n","                               dec_padding_mask=None)\n","\n","fn_out.shape  # (batch_size, tar_seq_len, target_vocab_size)\n"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["TensorShape([64, 36, 8000])"]},"metadata":{"tags":[]},"execution_count":69}]},{"cell_type":"code","metadata":{"id":"hlNWQ1Xeulrz"},"source":["def create_masks(inp, tar):\n","    # Encoder padding mask\n","    enc_padding_mask = create_padding_mask(inp)\n","\n","    # Used in the 2nd attention block in the decoder.\n","    # This padding mask is used to mask the encoder outputs.\n","    dec_padding_mask = create_padding_mask(inp)\n","\n","    # Used in the 1st attention block in the decoder.\n","    # It is used to pad and mask future tokens in the input received by\n","    # the decoder.\n","    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n","    dec_target_padding_mask = create_padding_mask(tar)\n","    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n","\n","    return enc_padding_mask, combined_mask, dec_padding_mask"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Bk5xd30iC06e"},"source":["!rm -rf ./checkpoints/"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gDFDcugCulpY"},"source":["checkpoint_path = \"./checkpoints/train\"\n","\n","# Keyword arguments are set as attributes of this object, \n","# and are saved with the checkpoint. Values must be trackable objects.\n","ckpt = tf.train.Checkpoint(transformer=transformer,\n","                           optimizer=optimizer)\n","\n","ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n","\n","# if a checkpoint exists, restore the latest checkpoint.\n","if ckpt_manager.latest_checkpoint:\n","    ckpt.restore(ckpt_manager.latest_checkpoint)\n","    print('Latest checkpoint restored!!')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_jy8jAkfulm8"},"source":["# The @tf.function trace-compiles train_step into a TF graph for faster\n","# execution. The function specializes to the precise shape of the argument\n","# tensors. To avoid re-tracing due to the variable sequence lengths or variable\n","# batch sizes (the last batch is smaller), use input_signature to specify\n","# more generic shapes.\n","\n","train_step_signature = [\n","    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n","    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n","]\n","\n","\n","@tf.function(input_signature=train_step_signature)\n","def train_step(inp, tar):\n","    tar_inp = tar[:, :-1]\n","    tar_real = tar[:, 1:]\n","\n","    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n","\n","    with tf.GradientTape() as tape:\n","        predictions, _ = transformer(inp, tar_inp,\n","                                        True,\n","                                        enc_padding_mask,\n","                                        combined_mask,\n","                                        dec_padding_mask)\n","        # print(inp)\n","        # print(tar_inp)\n","        # print(predictions)\n","        \n","        loss = loss_function(tar_real, predictions)\n","\n","        # if batch % 200 ==0:\n","        #     ru_sent = tokenizers.ru.detokenize(inp)\n","        #     ru_sent = ru_sent.numpy()[0].decode('utf-8')\n","        #     print(ru_sent)\n","\n","        #     en_sent = tokenizers.en.detokenize(tar)\n","        #     print(en_sent.numpy()[0].decode('utf-8'))\n","\n","        #     text, _, _ = evaluate(ru_sent, max_length=40)\n","        #     print(text)\n","        #     print()\n","\n","    \n","    gradients = tape.gradient(loss, transformer.trainable_variables)\n","    # print(gradients[0])\n","    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n","\n","    train_loss(loss)\n","    train_accuracy(accuracy_function(tar_real, predictions))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"Yq74gDPVulkM","executionInfo":{"status":"error","timestamp":1623897525181,"user_tz":360,"elapsed":11838037,"user":{"displayName":"Tim Sullivan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgDOmSXGsobJcA9UOtHRwU-sWjSZffSAqqvuM2IHQ=s64","userId":"11783081476684672772"}},"outputId":"fc0868d1-2ab0-4c86-8c14-ea3aa72c5698"},"source":["import time\n","\n","EPOCHS = 20\n","step = 0\n","for epoch in range(EPOCHS):\n","    start = time.time()\n","\n","    train_loss.reset_states()\n","    train_accuracy.reset_states()\n","\n","    # inp -> russian, tar -> english\n","    for (batch, (inp, tar)) in enumerate(train_batches):\n","        train_step(inp, tar)\n","\n","        if batch % 50 == 0:\n","            print(f'Epoch {epoch + 1} Batch {batch} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')\n","\n","    ckpt_save_path = ckpt_manager.save()\n","    \n","    print(f'Saving checkpoint for epoch {epoch+1} at {ckpt_save_path}')\n","    print(f'Epoch {epoch + 1} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')\n","    print(f'Time taken for 1 epoch: {time.time() - start:.2f} secs\\n')\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["так вот , этот подход делает возможным такие вещи как — прочесывание всех доступных данных из самых разных источников для определения ключевых закономерностеи и сведения их воедино . то , что невозможно было сделать раньше .\n","now this approach makes possible things like combing through all available data from very different sources , identifying key relationships and putting them in one place , something that ' s been nearly impossible to do before .\n","tf.Tensor(\n","[[   2 1759 4613 4613 4613 4613 4613 4613 4613 4613 4613 4613 4613 4613\n","  4613 4613 4613 4613 4613 4613 4613 4613 4613 4613 4613 4613 4613 4613\n","  4613 4613 4613 4613 4613 4613 4613 4613 4613 4613 4613 4613 4613]], shape=(1, 41), dtype=int64)\n","tf.Tensor(b'loss perceived perceived perceived perceived perceived perceived perceived perceived perceived perceived perceived perceived perceived perceived perceived perceived perceived perceived perceived perceived perceived perceived perceived perceived perceived perceived perceived perceived perceived perceived perceived perceived perceived perceived perceived perceived perceived perceived perceived', shape=(), dtype=string)\n","\n","Epoch 1 Batch 0 Loss 8.9408 Accuracy 0.0013\n","Epoch 1 Batch 50 Loss 8.8433 Accuracy 0.0166\n","Epoch 1 Batch 100 Loss 8.7305 Accuracy 0.0293\n","Epoch 1 Batch 150 Loss 8.6003 Accuracy 0.0348\n","хорошо , понятно , что всегда наидутся люди которые скажут , что та или иная картина была неправильно прибрана . итак мы можем провести с вами небольшои эксперимент .\n","ok , i mean , i can see there are always people that like reacting that one or another picture has n ' t been properly tidied up . so we can make a short test with you .\n","tf.Tensor([[2 3]], shape=(1, 2), dtype=int64)\n","tf.Tensor(b'', shape=(), dtype=string)\n","\n","Epoch 1 Batch 200 Loss 8.4423 Accuracy 0.0371\n","Epoch 1 Batch 250 Loss 8.2604 Accuracy 0.0381\n","Epoch 1 Batch 300 Loss 8.0640 Accuracy 0.0389\n","Epoch 1 Batch 350 Loss 7.8648 Accuracy 0.0407\n","прошли годы , и многие приключения , о которых я фантазировала ребенком — путешествия и прокладывание пути сквозь миры , отличающиеся от моего — стали реальными , благодаря моеи работе фотографа - документалиста .\n","years have passed , but many of the adventures i fantasized about as a child — traveling and weaving my way between worlds other than my own — have become realities through my work as a documentary photographer .\n","tf.Tensor(\n","[[ 2 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14\n","  14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14]], shape=(1, 41), dtype=int64)\n","tf.Tensor(b', , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,', shape=(), dtype=string)\n","\n","Epoch 1 Batch 400 Loss 7.6770 Accuracy 0.0480\n","Epoch 1 Batch 450 Loss 7.5166 Accuracy 0.0558\n","Epoch 1 Batch 500 Loss 7.3738 Accuracy 0.0633\n","Epoch 1 Batch 550 Loss 7.2470 Accuracy 0.0713\n","они в списке мировых долгожителеи .\n","they ' re the world ' s oldest living thing .\n","tf.Tensor([[ 2 85 85 84 14 85 84 14 85 84 16  3]], shape=(1, 12), dtype=int64)\n","tf.Tensor(b'and and the , and the , and the .', shape=(), dtype=string)\n","\n","Epoch 1 Batch 600 Loss 7.1321 Accuracy 0.0792\n","Epoch 1 Batch 650 Loss 7.0207 Accuracy 0.0869\n","Epoch 1 Batch 700 Loss 6.9209 Accuracy 0.0934\n","Epoch 1 Batch 750 Loss 6.8281 Accuracy 0.0995\n","это феодализм : один владелец , много рабочих .\n","this is feudalism : one owner , many workers .\n","tf.Tensor(\n","[[  2  85  47   9  57  84  39 157  14  85  84 157  14  85  84 157  14  85\n","   84 157  14  85  84  39  39 157  16   3]], shape=(1, 28), dtype=int64)\n","tf.Tensor(b\"and i ' s the a world , and the world , and the world , and the world , and the a a world .\", shape=(), dtype=string)\n","\n","Epoch 1 Batch 800 Loss 6.7403 Accuracy 0.1053\n","Epoch 1 Batch 850 Loss 6.6603 Accuracy 0.1107\n","Epoch 1 Batch 900 Loss 6.5861 Accuracy 0.1158\n","Epoch 1 Batch 950 Loss 6.5169 Accuracy 0.1205\n","у меня от этого столько энергии , что мне нужно наити для нее выход .\n","this gives me so much energy , and i ' ve got to have an outlet for all that energy .\n","tf.Tensor(\n","[[  2  47   9  57  39 157  14  85  47   9  57  39 157  14  85  47   9  57\n","   39 157  16   3]], shape=(1, 22), dtype=int64)\n","WARNING:tensorflow:5 out of the last 27 calls to <function CustomTokenizer.lookup at 0x7f9fc81a1290> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:tensorflow:5 out of the last 27 calls to <function CustomTokenizer.lookup at 0x7f9fc81a1290> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"],"name":"stderr"},{"output_type":"stream","text":["tf.Tensor(b\"i ' s a world , and i ' s a world , and i ' s a world .\", shape=(), dtype=string)\n","\n","Epoch 1 Batch 1000 Loss 6.4512 Accuracy 0.1250\n","Epoch 1 Batch 1050 Loss 6.3912 Accuracy 0.1291\n","Epoch 1 Batch 1100 Loss 6.3349 Accuracy 0.1332\n","Epoch 1 Batch 1150 Loss 6.2813 Accuracy 0.1371\n","и он усугубляет положение , сказав : « вообще - то нигде » .\n","` ` and he makes it worse by saying , ` ` actually , i do n ' t have a place . ' ' ' '\n","tf.Tensor(\n","[[ 2 38 38 38 38 38 38 38 38 38 38 38 38 38 38 38 38 38 38 38 38 38 38 38\n","  38 38 38 38 38 38 38 38 38 38 38 38 38 38 38 38 38]], shape=(1, 41), dtype=int64)\n","tf.Tensor(b'` ` ` ` ` ` ` ` ` ` ` ` ` ` ` ` ` ` ` ` ` ` ` ` ` ` ` ` ` ` ` ` ` ` ` ` ` ` ` `', shape=(), dtype=string)\n","\n","Epoch 1 Batch 1200 Loss 6.2298 Accuracy 0.1410\n","Epoch 1 Batch 1250 Loss 6.1840 Accuracy 0.1445\n","Epoch 1 Batch 1300 Loss 6.1397 Accuracy 0.1477\n","Epoch 1 Batch 1350 Loss 6.0965 Accuracy 0.1509\n","с тех пор я решил , что не вправе утешать умирающих при помощи лжи .\n","from that moment forward , i decided it was not my place to comfort the dying with my lies .\n","tf.Tensor(\n","[[  2  47   9  51  39 210  87  84 157  14  47   9  51 139  86 110  39 210\n","   87  84 157  16   3]], shape=(1, 23), dtype=int64)\n","WARNING:tensorflow:6 out of the last 29 calls to <function CustomTokenizer.lookup at 0x7f9fc81a1290> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:tensorflow:6 out of the last 29 calls to <function CustomTokenizer.lookup at 0x7f9fc81a1290> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"],"name":"stderr"},{"output_type":"stream","text":["tf.Tensor(b\"i ' m a lot of the world , i ' m going to be a lot of the world .\", shape=(), dtype=string)\n","\n","Epoch 1 Batch 1400 Loss 6.0563 Accuracy 0.1539\n","Epoch 1 Batch 1450 Loss 6.0176 Accuracy 0.1569\n","Epoch 1 Batch 1500 Loss 5.9833 Accuracy 0.1594\n","Epoch 1 Batch 1550 Loss 5.9491 Accuracy 0.1620\n","взгляните на это , это не основано на статистике .\n","you take a look at this , this is not based on statistics .\n","tf.Tensor([[  2  95  91   9 117 112 112  39 131 131 131 131  16   3]], shape=(1, 14), dtype=int64)\n","tf.Tensor(b\"so we ' re not not a very very very very .\", shape=(), dtype=string)\n","\n","Epoch 1 Batch 1600 Loss 5.9165 Accuracy 0.1643\n","Epoch 1 Batch 1650 Loss 5.8846 Accuracy 0.1668\n","Epoch 1 Batch 1700 Loss 5.8552 Accuracy 0.1689\n","Epoch 1 Batch 1750 Loss 5.8253 Accuracy 0.1712\n","археология дает нам возможность изучать древние цивилизации и увидеть в чем они преуспели , а в чем потерпели неудачу .\n","because archaeology gives us an opportunity to study past civilizations , and see where they succeeded and where they failed .\n","tf.Tensor(\n","[[  2  85  91   9 117 139  86 110  39 210  87  84 157  14  85  91   9 117\n","  139  86 110 264  86 110 264  86 110 264  86 110 264  86 110 264  86 110\n","  264  86 110 264  86]], shape=(1, 41), dtype=int64)\n","tf.Tensor(b\"and we ' re going to be a lot of the world , and we ' re going to be able to be able to be able to be able to be able to be able to be able to\", shape=(), dtype=string)\n","\n","Epoch 1 Batch 1800 Loss 5.7975 Accuracy 0.1732\n","Epoch 1 Batch 1850 Loss 5.7722 Accuracy 0.1750\n","Epoch 1 Batch 1900 Loss 5.7477 Accuracy 0.1768\n","Epoch 1 Batch 1950 Loss 5.7229 Accuracy 0.1787\n","и когда он останавливается напротив дома , он останавливается напротив вашего дома .\n","and when he stops in front of a house , he stops in front of your house .\n","tf.Tensor(\n","[[  2  85 127  97  39 192 290  87  84 157  14  85 127  97  39 192 290  87\n","   84 157  16   3]], shape=(1, 22), dtype=int64)\n","tf.Tensor(b'and he was a little bit of the world , and he was a little bit of the world .', shape=(), dtype=string)\n","\n","Epoch 1 Batch 2000 Loss 5.6992 Accuracy 0.1805\n","Epoch 1 Batch 2050 Loss 5.6755 Accuracy 0.1824\n","Epoch 1 Batch 2100 Loss 5.6542 Accuracy 0.1839\n","Epoch 1 Batch 2150 Loss 5.6333 Accuracy 0.1856\n","потрясающая презентация карен армстронг напомнила мне о том , что религия в правильном понимании этого слова - это не убеждения , а поведение .\n","and i was reminded by karen armstrong ' s fantastic presentation that religion really properly understood is not about belief , but about behavior .\n","tf.Tensor(\n","[[  2  85  47   9  51 139  86 105  52   9  58 101  86 110  39 192 290  87\n","   84 157  14  85  47   9  51 139  86 110  39 192 290  87  84 157  16   3]], shape=(1, 36), dtype=int64)\n","tf.Tensor(b\"and i ' m going to do n ' t have to be a little bit of the world , and i ' m going to be a little bit of the world .\", shape=(), dtype=string)\n","\n","Epoch 1 Batch 2200 Loss 5.6123 Accuracy 0.1872\n","Epoch 1 Batch 2250 Loss 5.5917 Accuracy 0.1888\n","Epoch 1 Batch 2300 Loss 5.5723 Accuracy 0.1903\n","Epoch 1 Batch 2350 Loss 5.5538 Accuracy 0.1917\n","есть очень много суперстимулов для сексуальности .\n","there ' s lots of supernormal stimuli for sexiness .\n","tf.Tensor([[  2  90   9  57  39 210  87 118  16   3]], shape=(1, 10), dtype=int64)\n","tf.Tensor(b\"it ' s a lot of people .\", shape=(), dtype=string)\n","\n","Epoch 1 Batch 2400 Loss 5.5356 Accuracy 0.1931\n","Epoch 1 Batch 2450 Loss 5.5185 Accuracy 0.1944\n","Epoch 1 Batch 2500 Loss 5.5013 Accuracy 0.1957\n","Epoch 1 Batch 2550 Loss 5.4839 Accuracy 0.1971\n","поэтому даже если опухоль расположена не на поверхности , вы все равно можете видеть ее .\n","so even if the tumor is not right on the surface , you ' ll still be able to see it .\n","tf.Tensor([[  2  92 107 145  14  92   9 117 112  39 210  87  84 157  16   3]], shape=(1, 16), dtype=int64)\n","tf.Tensor(b\"you can see , you ' re not a lot of the world .\", shape=(), dtype=string)\n","\n","Epoch 1 Batch 2600 Loss 5.4683 Accuracy 0.1983\n","Epoch 1 Batch 2650 Loss 5.4524 Accuracy 0.1995\n","Epoch 1 Batch 2700 Loss 5.4372 Accuracy 0.2006\n","Epoch 1 Batch 2750 Loss 5.4229 Accuracy 0.2016\n","но пока мы думали , что все шло хорошо , все снова было просто фантастическим , мы наткнулись еще на одну проблему . мужчины стали видеть явные изменения в своих женах .\n","but then while we were thinking everything was going well , once again everything was fantastic , we found our next setback : a lot of men started seeing the visible changes in their wife .\n","tf.Tensor(\n","[[  2  85  91   9 117 139  86 105  52   9  58 101  39 210  87 118  14  85\n","   91   9 117 139  86 105  94  14  85  91   9 117 139  86 105  94  14  85\n","   91   9 117 139  86]], shape=(1, 41), dtype=int64)\n","tf.Tensor(b\"and we ' re going to do n ' t have a lot of people , and we ' re going to do this , and we ' re going to do this , and we ' re going to\", shape=(), dtype=string)\n","\n","Epoch 1 Batch 2800 Loss 5.4085 Accuracy 0.2028\n","Epoch 1 Batch 2850 Loss 5.3942 Accuracy 0.2039\n","Epoch 1 Batch 2900 Loss 5.3800 Accuracy 0.2049\n","Epoch 1 Batch 2950 Loss 5.3659 Accuracy 0.2060\n","я так рада быть здесь .\n","i am so excited to be here .\n","tf.Tensor([[  2  47   9  51 139  86 110  39 192 290  16   3]], shape=(1, 12), dtype=int64)\n","tf.Tensor(b\"i ' m going to be a little bit .\", shape=(), dtype=string)\n","\n","Epoch 1 Batch 3000 Loss 5.3524 Accuracy 0.2071\n","Epoch 1 Batch 3050 Loss 5.3399 Accuracy 0.2080\n","Epoch 1 Batch 3100 Loss 5.3270 Accuracy 0.2090\n","Epoch 1 Batch 3150 Loss 5.3141 Accuracy 0.2100\n","мы провели огромное исследование и опрашивали людеи с разными транспортными службами , пытаясь выяснить , кто же изменил [ мнение ] и куда он поехал ?\n","well , so we did this huge interview survey with lots of travel services , and tried to figure out who changed , and where did they go ?\n","tf.Tensor(\n","[[  2  95  14 103   9  57 139  86 105  14  85  91   9 117 139  86 155  86\n","  110 264  86 155  39 210  87  84 157  14  85 152  91   9 117 139  86 155\n","   86 110 264  86 155]], shape=(1, 41), dtype=int64)\n","tf.Tensor(b\"so , what ' s going to do , and we ' re going to get to be able to get a lot of the world , and then we ' re going to get to be able to get\", shape=(), dtype=string)\n","\n","Epoch 1 Batch 3200 Loss 5.3017 Accuracy 0.2110\n","Epoch 1 Batch 3250 Loss 5.2904 Accuracy 0.2118\n","Epoch 1 Batch 3300 Loss 5.2787 Accuracy 0.2127\n","Epoch 1 Batch 3350 Loss 5.2673 Accuracy 0.2135\n","в свете этого , поэзия деиствительно исключительно хорошо справляется с определенными вещами .\n","that said , poetry does seem to be especially good at certain things .\n","tf.Tensor([[  2  85  90   9  57  39 131 263 193  88   9  57  39 131 263  16   3]], shape=(1, 17), dtype=int64)\n","tf.Tensor(b\"and it ' s a very important thing that ' s a very important .\", shape=(), dtype=string)\n","\n","Epoch 1 Batch 3400 Loss 5.2563 Accuracy 0.2143\n","Epoch 1 Batch 3450 Loss 5.2451 Accuracy 0.2151\n","Epoch 1 Batch 3500 Loss 5.2346 Accuracy 0.2160\n","Epoch 1 Batch 3550 Loss 5.2243 Accuracy 0.2167\n","( смех ) следующая история называется « коллекция хаверписа » . ничем не примечательныи склад , которыи на мгновение можно увидеть с трассы северного направления автомагистрали прикушко , используется как временное хранилище для коллекции хаверписа европеиских сухофруктов .\n","` ` ( laughter ) the next story is called ` ` ' ' the haverpiece collection ' ' ' ' a nondescript warehouse , visible for a moment from the northbound lanes of the prykushko expressway , serves as the temporary resting place for the haverpiece collection of european dried fruit . ' '\n","tf.Tensor(\n","[[  2  10 171  11  85  84 181 193  93  88  84 181 193  93  88  84 181 193\n","   93  88  84 181 193  93  88  84 181 193  93  88  84 181 193  93  88  84\n","  181 193  93  88  84]], shape=(1, 41), dtype=int64)\n","tf.Tensor(b'( laughter ) and the first thing is that the first thing is that the first thing is that the first thing is that the first thing is that the first thing is that the first thing is that the', shape=(), dtype=string)\n","\n","Epoch 1 Batch 3600 Loss 5.2145 Accuracy 0.2174\n","Epoch 1 Batch 3650 Loss 5.2046 Accuracy 0.2181\n","Epoch 1 Batch 3700 Loss 5.1950 Accuracy 0.2189\n","Epoch 1 Batch 3750 Loss 5.1857 Accuracy 0.2196\n","что же мы видим ?\n","what do we find ?\n","tf.Tensor([[  2 135 107  91 105  31   3]], shape=(1, 7), dtype=int64)\n","tf.Tensor(b'how can we do ?', shape=(), dtype=string)\n","\n","Epoch 1 Batch 3800 Loss 5.1761 Accuracy 0.2203\n","Epoch 1 Batch 3850 Loss 5.1671 Accuracy 0.2210\n","Epoch 1 Batch 3900 Loss 5.1578 Accuracy 0.2217\n","Epoch 1 Batch 3950 Loss 5.1488 Accuracy 0.2224\n","которои продвигался огромными скачками . к 2030 - му году вычислительные возможности на единицу цены увеличатся в 1 миллион раз .\n","there have been huge leaps . there will be a million - fold improvement in what you can get for the same price in computing by 2030 .\n","tf.Tensor(\n","[[  2  84 181 193  93  88  84 181  87  84 181  87  84 181  87  84 494 450\n","   93  88  84 181  87  84  59  16  57  16  57  16  57  16  57  16  57  16\n","    3]], shape=(1, 37), dtype=int64)\n","tf.Tensor(b'the first thing is that the first of the first of the first of the united states is that the first of the u . s . s . s . s . s .', shape=(), dtype=string)\n","\n","Epoch 1 Batch 4000 Loss 5.1406 Accuracy 0.2230\n","Epoch 1 Batch 4050 Loss 5.1315 Accuracy 0.2236\n","Epoch 1 Batch 4100 Loss 5.1235 Accuracy 0.2242\n","Epoch 1 Batch 4150 Loss 5.1146 Accuracy 0.2249\n","измерения мы проводили с помощью шкалы , придуманнои другими психологами , в которои участников опрашивали , какова вероятность вызвать у них отвращение в различных приведенных ситуациях .\n","the way that we measured this was by a scale that was constructed by some other psychologists that simply asked people across a wide variety of situations how likely they are to feel disgust .\n","tf.Tensor(\n","[[  2  91   9 117 139  86 110 264  86 110 264  86 110 264  86 110 264  86\n","  110 264  86 110 264  86 110 264  86 110 264  86 110 264  86 110 264  86\n","  110 264  86 110 264]], shape=(1, 41), dtype=int64)\n","tf.Tensor(b\"we ' re going to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able\", shape=(), dtype=string)\n","\n","Epoch 1 Batch 4200 Loss 5.1060 Accuracy 0.2256\n","Epoch 1 Batch 4250 Loss 5.0976 Accuracy 0.2262\n","Epoch 1 Batch 4300 Loss 5.0898 Accuracy 0.2268\n","Epoch 1 Batch 4350 Loss 5.0820 Accuracy 0.2273\n","тем не менее , он запустил самое прогрессивное в мире преобразование страны .\n","but he nevertheless pulled off one of the most progressive transformations any country has ever seen .\n","tf.Tensor(\n","[[  2  90   9  57  39 210  87  84 157  14  85  90   9  57 112  39 210  87\n","   84 157  16   3]], shape=(1, 22), dtype=int64)\n","tf.Tensor(b\"it ' s a lot of the world , and it ' s not a lot of the world .\", shape=(), dtype=string)\n","\n","Epoch 1 Batch 4400 Loss 5.0738 Accuracy 0.2279\n","Epoch 1 Batch 4450 Loss 5.0661 Accuracy 0.2285\n","Epoch 1 Batch 4500 Loss 5.0584 Accuracy 0.2291\n","Epoch 1 Batch 4550 Loss 5.0513 Accuracy 0.2296\n","эти фотографии прислали мне посетители .\n","these are some pictures visitors sent to me .\n","tf.Tensor([[  2  47   9  51 385 109 136  16   3]], shape=(1, 9), dtype=int64)\n","tf.Tensor(b\"i ' m talking about them .\", shape=(), dtype=string)\n","\n","Epoch 1 Batch 4600 Loss 5.0436 Accuracy 0.2302\n","Epoch 1 Batch 4650 Loss 5.0361 Accuracy 0.2308\n","Epoch 1 Batch 4700 Loss 5.0287 Accuracy 0.2313\n","Epoch 1 Batch 4750 Loss 5.0213 Accuracy 0.2319\n","лекция сильвии эрл под названием < > обеспечит нас рычагами и возможностью затронуть сердца людеи , у которых почти нет представления о местах , находящихся за пределами их собственного мира обитания . но мы будем надеяться , что их заинтересуют жизненные циклы таких существ как морские черепахи , которые не смогут выжить без открытых мореи .\n","sylvia ' s wish provides us with that leverage , that access to the heart of human beings , you might say , who have rarely seen places beyond their own toes , but are now hopefully going to become interested in the full life - cycle of creatures like these sea turtles , who indeed spend most of their time in the high seas .\n","tf.Tensor(\n","[[  2  91   9 153 194  39 210  87 118 147 101  39 210  87 118 147 101  39\n","  210  87 118 147 101  39 210  87 118 147 101  39 210  87 118 147 101  39\n","  210  87 118  14  85]], shape=(1, 41), dtype=int64)\n","tf.Tensor(b\"we ' ve got a lot of people who have a lot of people who have a lot of people who have a lot of people who have a lot of people who have a lot of people , and\", shape=(), dtype=string)\n","\n","Epoch 1 Batch 4800 Loss 5.0147 Accuracy 0.2324\n","Epoch 1 Batch 4850 Loss 5.0078 Accuracy 0.2328\n","Epoch 1 Batch 4900 Loss 5.0013 Accuracy 0.2333\n","Epoch 1 Batch 4950 Loss 4.9946 Accuracy 0.2338\n","это много времени .\n","that is a lot of time .\n","tf.Tensor([[  2  90   9  57  39 210  87 156  16   3]], shape=(1, 10), dtype=int64)\n","tf.Tensor(b\"it ' s a lot of time .\", shape=(), dtype=string)\n","\n","Epoch 1 Batch 5000 Loss 4.9876 Accuracy 0.2343\n","Epoch 1 Batch 5050 Loss 4.9807 Accuracy 0.2349\n","Epoch 1 Batch 5100 Loss 4.9740 Accuracy 0.2354\n","Epoch 1 Batch 5150 Loss 4.9676 Accuracy 0.2358\n","как я уже говорил , это были настоящие авантюристы .\n","as i said , they were adventurous people .\n","tf.Tensor(\n","[[  2  85  47   9  51 139  86 241  92  88  47   9  51 139  86 105  94  16\n","    3]], shape=(1, 19), dtype=int64)\n","tf.Tensor(b\"and i ' m going to tell you that i ' m going to do this .\", shape=(), dtype=string)\n","\n","Epoch 1 Batch 5200 Loss 4.9613 Accuracy 0.2363\n","Epoch 1 Batch 5250 Loss 4.9552 Accuracy 0.2368\n","Epoch 1 Batch 5300 Loss 4.9489 Accuracy 0.2372\n","Epoch 1 Batch 5350 Loss 4.9425 Accuracy 0.2377\n","когда мы конструировали первые установки , я месяцами проводил половину своего времени в магазине электроники .\n","i spent half my life at our local hardware store during the months when we built these units originally .\n","tf.Tensor(\n","[[  2  91   9 117 139  86 176 198  86  84 181 228  14  85 152  91   9 117\n","  139  86 176 198  86  84 181 228  16   3]], shape=(1, 28), dtype=int64)\n","tf.Tensor(b\"we ' re going to go back to the first day , and then we ' re going to go back to the first day .\", shape=(), dtype=string)\n","\n","Epoch 1 Batch 5400 Loss 4.9367 Accuracy 0.2381\n","Epoch 1 Batch 5450 Loss 4.9306 Accuracy 0.2386\n","Epoch 1 Batch 5500 Loss 4.9246 Accuracy 0.2391\n","Epoch 1 Batch 5550 Loss 4.9181 Accuracy 0.2396\n","аб : спасибо .\n","ab : thanks .\n","tf.Tensor([[  2 238  92  16   3]], shape=(1, 5), dtype=int64)\n","tf.Tensor(b'thank you .', shape=(), dtype=string)\n","\n","Epoch 1 Batch 5600 Loss 4.9119 Accuracy 0.2401\n","Epoch 1 Batch 5650 Loss 4.9062 Accuracy 0.2405\n","Epoch 1 Batch 5700 Loss 4.9005 Accuracy 0.2409\n","Epoch 1 Batch 5750 Loss 4.8945 Accuracy 0.2414\n","сбои компьютера , испорченныи провод , искрящии конвертор .\n","there was a computer spook , a broken wire , a converter that sparked .\n","tf.Tensor(\n","[[  2  90   9  57  39 210  87  84  54 311 379 646  14  85  90   9  57  39\n","  210  87  84  54 311 236  16   3]], shape=(1, 26), dtype=int64)\n","tf.Tensor(b\"it ' s a lot of the palig , and it ' s a lot of the paly .\", shape=(), dtype=string)\n","\n","Epoch 1 Batch 5800 Loss 4.8889 Accuracy 0.2418\n","Epoch 1 Batch 5850 Loss 4.8837 Accuracy 0.2422\n","Epoch 1 Batch 5900 Loss 4.8778 Accuracy 0.2426\n","Epoch 1 Batch 5950 Loss 4.8723 Accuracy 0.2431\n","« я был бы не против жить в стране с исламским фундаментализмом » .\n","` ` ` ` i would n ' t mind living in a fundamentalist islamic state . ' ' ' '\n","tf.Tensor(\n","[[  2  38  38  47   9 153 173  39 413 745  89  84 157  14  85  47   9  51\n","  139  86 110  39 188 328  16   9   9   9   3]], shape=(1, 29), dtype=int64)\n","tf.Tensor(b\"` ` i ' ve been a young girl in the world , and i ' m going to be a new country . ' ' '\", shape=(), dtype=string)\n","\n","Epoch 1 Batch 6000 Loss 4.8669 Accuracy 0.2435\n","Epoch 1 Batch 6050 Loss 4.8614 Accuracy 0.2439\n","Epoch 1 Batch 6100 Loss 4.8558 Accuracy 0.2444\n","Epoch 1 Batch 6150 Loss 4.8508 Accuracy 0.2448\n","но в процессе нашего анализа мы выяснили , что корень обеих проблем , мы выяснили , что корень обеих проблем , лежит в основах управления .\n","but as we were doing our analysis we realized that there was a common root cause to these two issues that relates , in fact , to the basic pillars of management .\n","tf.Tensor(\n","[[  2 102  91   9 153 173 330 104  84 157  14  85  91   9 153 194  39 210\n","   87 118 147  99 139  86 110 264  86 110 264  86 105  88  16   3]], shape=(1, 34), dtype=int64)\n","tf.Tensor(b\"but we ' ve been working on the world , and we ' ve got a lot of people who are going to be able to be able to do that .\", shape=(), dtype=string)\n","\n","Epoch 1 Batch 6200 Loss 4.8455 Accuracy 0.2452\n","Epoch 1 Batch 6250 Loss 4.8401 Accuracy 0.2456\n","Epoch 1 Batch 6300 Loss 4.8349 Accuracy 0.2460\n","Epoch 1 Batch 6350 Loss 4.8298 Accuracy 0.2464\n","они – лучше всех .\n","singapore is the best one .\n","tf.Tensor([[  2  96   9 117 114 139  86 110 131 263  16   3]], shape=(1, 12), dtype=int64)\n","tf.Tensor(b\"they ' re all going to be very important .\", shape=(), dtype=string)\n","\n","Epoch 1 Batch 6400 Loss 4.8253 Accuracy 0.2468\n","Epoch 1 Batch 6450 Loss 4.8206 Accuracy 0.2472\n","Epoch 1 Batch 6500 Loss 4.8153 Accuracy 0.2476\n","выше нос ! посмотри на меня !\n","hold your head up . look at me .\n","tf.Tensor([[  2 116 571  14  96   9 117 112  16   3]], shape=(1, 10), dtype=int64)\n","tf.Tensor(b\"my friends , they ' re not .\", shape=(), dtype=string)\n","\n","Epoch 2 Batch 0 Loss 4.0656 Accuracy 0.2946\n","Epoch 2 Batch 50 Loss 4.1762 Accuracy 0.2982\n","Epoch 2 Batch 100 Loss 4.1568 Accuracy 0.2990\n","Epoch 2 Batch 150 Loss 4.1723 Accuracy 0.2984\n","загвоздка заключалась в том , как они мне объяснили ситуацию , что создавать интерьеры надо посредством существующих технологии , и нет дополнительных средств для поточного производства .\n","so the problem became — and they set this dilemma to me — that you have to design the interior using only our existing technology , and there ' s no money for tooling or molding .\n","tf.Tensor(\n","[[  2  85  47   9  51 112 139  86 105  90  14  85  47   9  51 112 139  86\n","  110  39 208  87 166  86 105  90  14  85  96   9 117 112  39 208  87 166\n","   86 105  90  14  85]], shape=(1, 41), dtype=int64)\n","tf.Tensor(b\"and i ' m not going to do it , and i ' m not going to be a kind of way to do it , and they ' re not a kind of way to do it , and\", shape=(), dtype=string)\n","\n","Epoch 2 Batch 200 Loss 4.1554 Accuracy 0.3004\n","Epoch 2 Batch 250 Loss 4.1572 Accuracy 0.3003\n","Epoch 2 Batch 300 Loss 4.1596 Accuracy 0.2998\n","Epoch 2 Batch 350 Loss 4.1583 Accuracy 0.2999\n","напротив , те , кто деиствительно был в отеле , чьи тела деиствительно находились в пространстве отеля , подробно описывали пространственные признаки .\n","in contrast , the people that wrote the reviews that were actually there , their bodies actually entered the physical space , they talked a lot more about spatial information .\n","tf.Tensor(\n","[[   2   85   84  172  193   88   91    9  153  173  264   86  105   93\n","    88   84   41 3502 1198   87   84   41 3502 1198   87   84  368   14\n","    84   41 3502  633   14   84   41 3502  633   14   84   41 3502]], shape=(1, 41), dtype=int64)\n","tf.Tensor(b\"and the other thing that we ' ve been able to do is that the chack of the chack of the city , the chap , the chap , the cha\", shape=(), dtype=string)\n","\n","Epoch 2 Batch 400 Loss 4.1582 Accuracy 0.2998\n","Epoch 2 Batch 450 Loss 4.1606 Accuracy 0.2996\n","Epoch 2 Batch 500 Loss 4.1600 Accuracy 0.2996\n","Epoch 2 Batch 550 Loss 4.1594 Accuracy 0.2996\n","скажем , что должно случиться , чтобы началась пандемия ? »\n","to say ahh , if this happens then we are going to have a pandemic ?\n","tf.Tensor(\n","[[  2 103  93  84 300  88  93  88  84 300  93  88  84 357  93  84 179  31\n","    3]], shape=(1, 19), dtype=int64)\n","tf.Tensor(b'what is the question that is that the question is that the future is the right ?', shape=(), dtype=string)\n","\n","Epoch 2 Batch 600 Loss 4.1597 Accuracy 0.2996\n","Epoch 2 Batch 650 Loss 4.1559 Accuracy 0.3002\n","Epoch 2 Batch 700 Loss 4.1535 Accuracy 0.3004\n","Epoch 2 Batch 750 Loss 4.1494 Accuracy 0.3010\n","оно конечно . гармония - нет .\n","it is finite ; harmony is infinite .\n","tf.Tensor([[  2  90   9  57 112  39 192 290  16   3]], shape=(1, 10), dtype=int64)\n","tf.Tensor(b\"it ' s not a little bit .\", shape=(), dtype=string)\n","\n","Epoch 2 Batch 800 Loss 4.1488 Accuracy 0.3013\n","Epoch 2 Batch 850 Loss 4.1471 Accuracy 0.3016\n","Epoch 2 Batch 900 Loss 4.1459 Accuracy 0.3016\n","Epoch 2 Batch 950 Loss 4.1461 Accuracy 0.3016\n","женщины участвуют в сборе средств .\n","and women do fundraising .\n","tf.Tensor([[  2  84 296 147 101 173  89  84 328  16   3]], shape=(1, 11), dtype=int64)\n","tf.Tensor(b'the women who have been in the country .', shape=(), dtype=string)\n","\n","Epoch 2 Batch 1000 Loss 4.1437 Accuracy 0.3018\n","Epoch 2 Batch 1050 Loss 4.1436 Accuracy 0.3018\n","Epoch 2 Batch 1100 Loss 4.1428 Accuracy 0.3018\n","Epoch 2 Batch 1150 Loss 4.1408 Accuracy 0.3020\n","почему ? они не доверяют законам . почему они не доверяют законам ?\n","and why not ? they do n ' t trust the law . why do n ' t they trust the law ?\n","tf.Tensor([[  2 213  31 213  31 213  31 213  31 213  31  96  99 112  88  31   3]], shape=(1, 17), dtype=int64)\n","tf.Tensor(b'why ? why ? why ? why ? why ? they are not that ?', shape=(), dtype=string)\n","\n","Epoch 2 Batch 1200 Loss 4.1389 Accuracy 0.3021\n","Epoch 2 Batch 1250 Loss 4.1373 Accuracy 0.3021\n","Epoch 2 Batch 1300 Loss 4.1343 Accuracy 0.3025\n","Epoch 2 Batch 1350 Loss 4.1325 Accuracy 0.3026\n","и вот что мы хотим сделать . представьте , что , читая лекцию , можно одновременно говорить с людьми на их родном языке .\n","so here ' s what we think we want to do : imagine giving a lecture and being able to talk to people in their own native language simultaneously .\n","tf.Tensor(\n","[[  2  95  14  91   9 117 139  86 105  88  14  85  91   9 117 139  86 105\n","   88  14  85  91   9 117 139  86 110 264  86 105  88  14  85  91   9 117\n","  139  86 110 264  86]], shape=(1, 41), dtype=int64)\n","tf.Tensor(b\"so , we ' re going to do that , and we ' re going to do that , and we ' re going to be able to do that , and we ' re going to be able to\", shape=(), dtype=string)\n","\n","Epoch 2 Batch 1400 Loss 4.1304 Accuracy 0.3029\n","Epoch 2 Batch 1450 Loss 4.1299 Accuracy 0.3031\n","Epoch 2 Batch 1500 Loss 4.1291 Accuracy 0.3030\n","Epoch 2 Batch 1550 Loss 4.1249 Accuracy 0.3034\n","большая часть нашего культурного наследия заставляет оглядываться назад , идеализируя прошлое .\n","most of our cultural heritage has tended to look backward , romanticizing the past .\n","tf.Tensor(\n","[[  2  91   9 153 194  39 210  87 118 147 101 173 264  86 105  90  14  85\n","   91   9 153 194  86 110  39 188 166  87  84 538  16   3]], shape=(1, 32), dtype=int64)\n","tf.Tensor(b\"we ' ve got a lot of people who have been able to do it , and we ' ve got to be a new way of the internet .\", shape=(), dtype=string)\n","\n","Epoch 2 Batch 1600 Loss 4.1238 Accuracy 0.3035\n","Epoch 2 Batch 1650 Loss 4.1226 Accuracy 0.3036\n","Epoch 2 Batch 1700 Loss 4.1206 Accuracy 0.3038\n","Epoch 2 Batch 1750 Loss 4.1190 Accuracy 0.3040\n","глядя на это , я просто умираю со смеху , потому что знаю , что этот французскии мохер и все эти старинные немецкие ленты и шерсть я достал еще на мельнице в небраске и носил около 10 лет , а это древние китаиские юбки .\n","and i crack up at this piece , because when i see it i know that ' s french angora and all antique german ribbons and wool that i got in a nebraska mill and carried around for 10 years and then antique chinese skirts .\n","tf.Tensor(\n","[[  2  85  47   9  51 139  86 241  92  88  84 181 156  47   9  51 139  86\n","  145  88  84 181 193  93  88  84 200 193  47   9  51 139  86 105  93  88\n","   84 221 193  93  88]], shape=(1, 41), dtype=int64)\n","tf.Tensor(b\"and i ' m going to tell you that the first time i ' m going to see that the first thing is that the only thing i ' m going to do is that the same thing is that\", shape=(), dtype=string)\n","\n","Epoch 2 Batch 1800 Loss 4.1176 Accuracy 0.3042\n","Epoch 2 Batch 1850 Loss 4.1165 Accuracy 0.3043\n","Epoch 2 Batch 1900 Loss 4.1148 Accuracy 0.3044\n","Epoch 2 Batch 1950 Loss 4.1137 Accuracy 0.3045\n","с однои стороны , это звучит нелепо .\n","on the one hand , it sounds ridiculous .\n","tf.Tensor([[  2  85  84 200 193  93  88  90   9  57 112  39 192 290  16   3]], shape=(1, 16), dtype=int64)\n","tf.Tensor(b\"and the only thing is that it ' s not a little bit .\", shape=(), dtype=string)\n","\n","Epoch 2 Batch 2000 Loss 4.1111 Accuracy 0.3048\n","Epoch 2 Batch 2050 Loss 4.1100 Accuracy 0.3049\n","Epoch 2 Batch 2100 Loss 4.1083 Accuracy 0.3050\n","Epoch 2 Batch 2150 Loss 4.1066 Accuracy 0.3052\n","это этическая концепция .\n","this is an ethical conception .\n","tf.Tensor([[  2  94  93  84 181 193  16   3]], shape=(1, 8), dtype=int64)\n","tf.Tensor(b'this is the first thing .', shape=(), dtype=string)\n","\n","Epoch 2 Batch 2200 Loss 4.1047 Accuracy 0.3054\n","Epoch 2 Batch 2250 Loss 4.1022 Accuracy 0.3057\n","Epoch 2 Batch 2300 Loss 4.1017 Accuracy 0.3057\n","Epoch 2 Batch 2350 Loss 4.1000 Accuracy 0.3059\n","спасибо за такую реакцию .\n","thank you for that reaction .\n","tf.Tensor([[  2 238  92  16   3]], shape=(1, 5), dtype=int64)\n","tf.Tensor(b'thank you .', shape=(), dtype=string)\n","\n","Epoch 2 Batch 2400 Loss 4.0992 Accuracy 0.3059\n","Epoch 2 Batch 2450 Loss 4.0982 Accuracy 0.3060\n","Epoch 2 Batch 2500 Loss 4.0962 Accuracy 0.3062\n","Epoch 2 Batch 2550 Loss 4.0953 Accuracy 0.3063\n","это говорит о том , что моцарту следовало быть осторожнее при выборе сексуальных партнеров .\n","this suggests that mozart should have bit more careful , perhaps , when choosing his sexual partners .\n","tf.Tensor(\n","[[  2  90   9  57  39 242 193  88   9  57  39 242 193  86 105 106  84 463\n","   87  84 463  16   3]], shape=(1, 23), dtype=int64)\n","tf.Tensor(b\"it ' s a great thing that ' s a great thing to do with the public of the public .\", shape=(), dtype=string)\n","\n","Epoch 2 Batch 2600 Loss 4.0940 Accuracy 0.3064\n","Epoch 2 Batch 2650 Loss 4.0918 Accuracy 0.3067\n","Epoch 2 Batch 2700 Loss 4.0904 Accuracy 0.3068\n","Epoch 2 Batch 2750 Loss 4.0890 Accuracy 0.3069\n","таким образом , мы не знаем , зачем творить , но имеем много причин не делать этого .\n","we do n ' t know why we should be artists , but we have many reasons why we ca n ' t be .\n","tf.Tensor(\n","[[  2  85  91   9 117 112 139  86 110 131 215  14 102  91   9 117 112 126\n","  139  86 110 131 215  16   3]], shape=(1, 25), dtype=int64)\n","tf.Tensor(b\"and we ' re not going to be very good , but we ' re not just going to be very good .\", shape=(), dtype=string)\n","\n","Epoch 2 Batch 2800 Loss 4.0873 Accuracy 0.3070\n","Epoch 2 Batch 2850 Loss 4.0857 Accuracy 0.3072\n","Epoch 2 Batch 2900 Loss 4.0848 Accuracy 0.3073\n","Epoch 2 Batch 2950 Loss 4.0832 Accuracy 0.3075\n","я расскажу вам историю человека на этои фотографии .\n","the guy in the picture here , i ' ll tell you his story .\n","tf.Tensor([[  2  47   9 223 241  92  39 278 109  84 278  87  84 274 115  16   3]], shape=(1, 17), dtype=int64)\n","tf.Tensor(b\"i ' ll tell you a story about the story of the next one .\", shape=(), dtype=string)\n","\n","Epoch 2 Batch 3000 Loss 4.0818 Accuracy 0.3076\n","Epoch 2 Batch 3050 Loss 4.0804 Accuracy 0.3077\n","Epoch 2 Batch 3100 Loss 4.0791 Accuracy 0.3078\n","Epoch 2 Batch 3150 Loss 4.0779 Accuracy 0.3079\n","даваите ненадолго вернемся в 1819 год , в трудное положение команды китобоя эссекс .\n","well let ' s return to the year 1819 for a moment , to the situation facing the crew of the whaleship essex .\n","tf.Tensor(\n","[[  2  85  84 181 193  88  47   9  51 139  86 105  93  88  84 494 450  87\n","   84 494 450  93  88  84 494 450  93  52   9  58 139  86 110  84 181 416\n","  169  16   3]], shape=(1, 39), dtype=int64)\n","tf.Tensor(b\"and the first thing that i ' m going to do is that the united states of the united states is that the united states is n ' t going to be the first 20 years .\", shape=(), dtype=string)\n","\n","Epoch 2 Batch 3200 Loss 4.0767 Accuracy 0.3081\n","Epoch 2 Batch 3250 Loss 4.0754 Accuracy 0.3082\n","Epoch 2 Batch 3300 Loss 4.0742 Accuracy 0.3083\n","Epoch 2 Batch 3350 Loss 4.0732 Accuracy 0.3084\n","представьте себе , сколько - же будет стоить упомянутыи 100 - долларовыи компьютер в 2020 году в качестве инструмента для обучения .\n","now , just imagine what that $ 100 computer will be in 2020 as a tool for education .\n","tf.Tensor(\n","[[  2  95  14 391  84 274 226  14  84 188 685 368  93  86 110 264  86 155\n","   86 110 264  86 155  39 188 166  86 155  39 188 166  86 155  39 188 166\n","   86 155  39 226  16]], shape=(1, 41), dtype=int64)\n","tf.Tensor(b'so , imagine the next year , the new york city is to be able to get to