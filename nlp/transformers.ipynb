{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"transformers.ipynb","provenance":[],"collapsed_sections":["swDUGQDm7dfe","SmfNEG-_1j0r"],"mount_file_id":"1dAycfA3ldhHktD80zKUNXalR0QuSCadC","authorship_tag":"ABX9TyML7lHJO6R47ZXtQY3BuaWC"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"swDUGQDm7dfe"},"source":["# Transformers\n","\n","Paper: [Attention is all you need](https://arxiv.org/abs/1706.03762)\n","\n","Resources:\n","- [The Illustrated Transformer\n","](https://jalammar.github.io/illustrated-transformer/)\n","- [Visualizing A Neural Machine Translation Model (Mechanics of Seq2seq Models With Attention)](https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/)\n","- [TF Docs: Transformer model for language understanding\n","](https://www.tensorflow.org/text/tutorials/transformer)\n","\n","#### Summary\n","- Network based only on attention, without recurrence or convolutions.\n","- Addresses long sequence dependence problem of recurrent models with single encoder hidden state.\n","- Position information is included separately \n","\n","### Approach\n","\n","- Until done:\n","    1. Read the paper\n","    2. Try to implement until stuck\n","    3. Check resources\n","    4. Go to 1.\n","The end product will look a lot like the TF example, *but* I'll learn a lot along the way.\n","\n","### Results\n","- Tried both Portugese to English from the TF docs and Russian to English.\n","- The Russian to English did not perform as well given the time allocated for training and may need additional hyperparameter tuning.\n","\n","[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tims457/ml_notebooks/blob/main/nlp/transformers.ipynb)\n"]},{"cell_type":"code","metadata":{"id":"CGY6wp3dAJOq"},"source":["import tensorflow as tf\n","import numpy as np\n","import tensorflow_datasets as tfds\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8Zg0LYfqACrO"},"source":["# Attention model\n","\n","### Notation\n","- $Q$ = Query\n","- $K$ = Key, dimension $d_k$ = 64\n","- $V$ = Value, dimension $d_v$ = 64\n","- $n$ = input sequence length\n","- $m$ = output sequence length\n","- $d_\\text{model}$ = model dimension = 512\n","- $d_{ff}$ = inner layer dimension = 2048\n","- $h$ = heads\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Yt2vmmu7Ntjy"},"source":["## Attention\n","\n","### Basics\n","- Deals with problem of relationships between elements of a long sequence experienced by recurrent models.\n","- Pass all hidden states (all encoder output) to decoder.\n","- Decoder scores each of the encoder hidden states, applies a softmax to the score, then multiplies the score times the hidden state from the encoder. This increases the value of states that scored higher.\n","- Rather than having a single hidden state from a recurrent sequence, the decoder has *all* the hidden states and a score (or attention) value for each of them to weight their importance.\n","- Self-attention means the network is learning to associate the relationships between input elements. \n","\n","<img src=\"https://www.researchgate.net/publication/333078019/figure/fig1/AS:758304078839808@1557805189409/left-Scaled-Dot-Product-Attention-right-Multi-Head-Attention.png\" height=300 />"]},{"cell_type":"markdown","metadata":{"id":"N9EJUz9cK5fq"},"source":["## Multi-head attention\n","- Query, Key, Value (Q,K,V) for each of the encoder input vector created by the embedding.\n","- Attention value is the softmax of Query x Key scaled by the dimension $d_k$ then multiplied by Value\n","\n","$$\\text{Attention(Q,K,V)} = \\text{softmax}_k(\\frac{QK^T}{\\sqrt{d_k}})V$$\n","\n","- Multiple heads - number of attention layers running in parallel\n","- By starting with multiple heads (8 in this case) the encoder and decoder are really multiple encoders and decoders operating in parallel. \n","\n"]},{"cell_type":"markdown","metadata":{"id":"19f35FemNUHx"},"source":[""]},{"cell_type":"code","metadata":{"id":"YdGIapvC0VNx"},"source":["import tensorflow as tf\n","from tensorflow.keras import layers\n","\n","# Attention\n","# sec 3.2.1\n","def scaled_dot_product_attention(q, k, v, mask):\n","    # get dimensions of the input, cast from tensor to float\n","    d_k = tf.cast(tf.shape(k)[-1], tf.float32)\n","    \n","    # compute queries x keys and scale by dimension\n","    attention_logits = tf.matmul(q, k, transpose_b=True)\n","    \n","    scaled_attention_logits = attention_logits / tf.math.sqrt(d_k)\n","    # print(f\"scaled attention shape {scaled_attention_logits.shape}\")\n","\n","    # apply decoder mask\n","    if mask is not None:\n","        scaled_attention_logits += (mask * -1e9)\n","\n","    # normalize all scores\n","    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n","    # print(f\"attention shape {attention_weights.shape}\")\n","\n","    # times value\n","    output = tf.matmul(attention_weights, v)\n","    # print(f\"output shape {output.shape}\")\n","\n","    return output, attention_weights"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fq0feSF1aa-o","executionInfo":{"status":"ok","timestamp":1624123270577,"user_tz":360,"elapsed":63,"user":{"displayName":"Tim Sullivan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgDOmSXGsobJcA9UOtHRwU-sWjSZffSAqqvuM2IHQ=s64","userId":"11783081476684672772"}},"outputId":"e676b2d6-cfd4-4eb8-ba2e-d9ee1e5d1ae8"},"source":["def print_out(q, k, v):\n","    temp_out, temp_attn = scaled_dot_product_attention(q, k, v, None)\n","    print('Attention weights are:')\n","    print(np.round(temp_attn, decimals=2))\n","    print('Output is:')\n","    print(np.round(temp_out, decimals=2))\n","\n","temp_k = tf.constant([[10, 0, 0],\n","                      [0, 10, 0],\n","                      [0, 0, 10],\n","                      [0, 0, 10]], dtype=tf.float32)  # (4, 3)\n","\n","temp_v = tf.constant([[1, 0],\n","                      [10, 0],\n","                      [100, 5],\n","                      [1000, 6]], dtype=tf.float32)  # (4, 2)\n","\n","\n","# The dot product attention is selecting the key that aligns with the \n","# query and then returning the associated value.\n","temp_q = tf.constant([[0, 10, 0]], dtype=tf.float32)  # (1, 3)\n","print_out(temp_q, temp_k, temp_v)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Attention weights are:\n","[[0. 1. 0. 0.]]\n","Output is:\n","[[10.  0.]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"B1fsCrjh02cu"},"source":["Use the `scaled_dot_product_attention` layer to get a handle on how the model is selecting query-key pairs and computing their value.\n"]},{"cell_type":"code","metadata":{"id":"fcjJuOdxAHt7"},"source":["# sec 3.2.2\n","class MultiHeadAttention(tf.keras.layers.Layer):\n","    def __init__(self, d_model, num_heads):\n","        super(MultiHeadAttention, self).__init__()\n","        self.num_heads = num_heads\n","        self.d_model = d_model\n","\n","        assert self.d_model % self.num_heads == 0 \n","\n","        self.depth = self.d_model  // num_heads\n","\n","        self.wq = layers.Dense(d_model)\n","        self.wk = layers.Dense(d_model)\n","        self.wv = layers.Dense(d_model)\n","\n","        self.dense = layers.Dense(d_model)\n","\n","    def split_heads(self, x, batch_size):\n","        \"\"\"\n","        The inputs need to be reshaped in order to be fed into the attention portion.\n","        The model dimension d_k get split into heads x depth.\n","        Then transposed to (batch_size, num_heads, seq_len, depth)\n","\n","        \"\"\"\n","        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n","        return tf.transpose(x, perm=[0, 2, 1, 3])\n","\n","    # forward computation\n","    def call(self, v, k, q, mask):\n","        batch_size = tf.shape(q)[0]\n","\n","        q = self.wq(q) # (batch_size, seq_len, d_model)\n","        k = self.wk(k)\n","        v = self.wv(v)\n","\n","        q = self.split_heads(q, batch_size) # (batch_size, num_heads, seq_len_q, depth)\n","        k = self.split_heads(k, batch_size)\n","        v = self.split_heads(v, batch_size)\n","\n","        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n","        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n","        scaled_attention, attention_weights = scaled_dot_product_attention(q, k, v, mask)\n","\n","        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n","\n","        concat_attention = tf.reshape(scaled_attention,\n","                                    (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n","\n","        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n","\n","        return output, attention_weights\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1ZXWw8OQrT2T","executionInfo":{"status":"ok","timestamp":1624123270579,"user_tz":360,"elapsed":61,"user":{"displayName":"Tim Sullivan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgDOmSXGsobJcA9UOtHRwU-sWjSZffSAqqvuM2IHQ=s64","userId":"11783081476684672772"}},"outputId":"806beebd-073b-493c-cb10-f502967fa32c"},"source":["tf.random.set_seed(42)\n","mha = MultiHeadAttention(512, 8)\n","\n","x = tf.ones((1,60, 512))\n","out, attn = mha(x,k=x,q=x, mask=None)\n","out.shape, attn.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(TensorShape([1, 60, 512]), TensorShape([1, 8, 60, 60]))"]},"metadata":{"tags":[]},"execution_count":23}]},{"cell_type":"markdown","metadata":{"id":"txzzj49L_30r"},"source":["# Data\n","- Used English-German and English-French dataset. 4.5 million and 36 million sentences respectively\n","- 37000 word English-German vocab\n","- 32000 word English-French vocab\n","- Instead let's use the TF dataset for Russian to English"]},{"cell_type":"code","metadata":{"id":"xcFqTEr83YFW"},"source":["examples, metadata = tfds.load('ted_hrlr_translate/ru_to_en', with_info=True,\n","                               as_supervised=True)\n","train_examples, val_examples = examples['train'], examples['validation']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zkFVV1l956It","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1624123270915,"user_tz":360,"elapsed":19,"user":{"displayName":"Tim Sullivan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgDOmSXGsobJcA9UOtHRwU-sWjSZffSAqqvuM2IHQ=s64","userId":"11783081476684672772"}},"outputId":"9c3251d9-7515-4921-f32b-acf0fd8f3ec7"},"source":["#print  examples\n","for ru, en in train_examples.take(1):\n","  print(\"Russian: \", ru.numpy().decode('utf-8'))\n","  print(\"English:   \", en.numpy().decode('utf-8'))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Russian:  к : успех , перемены возможны только с оружием в руках .\n","English:    c : success , the change is only coming through the barrel of the gun .\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"7M6umRAu6e9t"},"source":["### Tokenization\n","- There's no tokenizer for the `ted hrlr` Russian to English set that I can find so I have to make one.\n","- The Google docs example for attention said they used a sub-word version built with Bert.\n","- I'll just use the example here https://www.tensorflow.org/text/guide/subwords_tokenizer"]},{"cell_type":"code","metadata":{"id":"TioATfFL8tdX"},"source":["!pip install -q -U tensorflow-text"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WFL54Yfg614k"},"source":["\n","import tensorflow_text as text\n","from tensorflow_text.tools.wordpiece_vocab import bert_vocab_from_dataset as bert_vocab\n","\n","train_en = train_examples.map(lambda ru, en: en)\n","train_ru = train_examples.map(lambda ru, en: ru)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Fjtfi7DkPM09"},"source":["This section will generate the English and Russain vocabulary, but it takes a long time so use the included `txt` files."]},{"cell_type":"code","metadata":{"id":"wFxzKMJi8xbD"},"source":["bert_tokenizer_params=dict(lower_case=True)\n","reserved_tokens=[\"[PAD]\", \"[UNK]\", \"[START]\", \"[END]\"]\n","\n","\n","# bert_vocab_args = dict(\n","#     # The target vocabulary size\n","#     vocab_size = 8000,\n","#     # Reserved tokens that must be included in the vocabulary\n","#     reserved_tokens=reserved_tokens,\n","#     # Arguments for `text.BertTokenizer`\n","#     bert_tokenizer_params=bert_tokenizer_params,\n","#     # Arguments for `wordpiece_vocab.wordpiece_tokenizer_learner_lib.learn`\n","#     learn_params={},\n","# )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"plzF-nFw9Bps"},"source":["# %%time\n","# en_vocab = bert_vocab.bert_vocab_from_dataset(\n","#     train_en.batch(1000).prefetch(2),\n","#     **bert_vocab_args\n","# )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"t2Hkjven9CSD"},"source":["# print(en_vocab[:10])\n","# print(en_vocab[100:110])\n","# print(en_vocab[1000:1010])\n","# print(en_vocab[-10:])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QDzCJsx-9CeJ"},"source":["# %%time\n","# ru_vocab = bert_vocab.bert_vocab_from_dataset(\n","#     train_ru.batch(1000).prefetch(2),\n","#     **bert_vocab_args\n","# )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NobcBT1P9CmQ"},"source":["# print(ru_vocab[:10])\n","# print(ru_vocab[100:110])\n","# print(ru_vocab[1000:1010])\n","# print(ru_vocab[-10:])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jSajpI4T9CuF"},"source":["# def write_vocab_file(filepath, vocab):\n","#   with open(filepath, 'w') as f:\n","#     for token in vocab:\n","#       print(token, file=f)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gYVqjS_b9C1J"},"source":["# write_vocab_file('en_vocab.txt', en_vocab)\n","# write_vocab_file('ru_vocab.txt', ru_vocab)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"n0pWFxlt9DDS"},"source":["# copying this module from the TF docs.\n","\n","import pathlib\n","import re\n","\n","START = tf.argmax(tf.constant(reserved_tokens) == \"[START]\")\n","END = tf.argmax(tf.constant(reserved_tokens) == \"[END]\")\n","\n","def cleanup_text(reserved_tokens, token_txt):\n","  # Drop the reserved tokens, except for \"[UNK]\".\n","  bad_tokens = [re.escape(tok) for tok in reserved_tokens if tok != \"[UNK]\"]\n","  bad_token_re = \"|\".join(bad_tokens)\n","\n","  bad_cells = tf.strings.regex_full_match(token_txt, bad_token_re)\n","  result = tf.ragged.boolean_mask(token_txt, ~bad_cells)\n","\n","  # Join them into strings.\n","  result = tf.strings.reduce_join(result, separator=' ', axis=-1)\n","\n","  return result\n","\n","def add_start_end(ragged):\n","  count = ragged.bounding_shape()[0]\n","  starts = tf.fill([count,1], START)\n","  ends = tf.fill([count,1], END)\n","  return tf.concat([starts, ragged, ends], axis=1)\n","\n","class CustomTokenizer(tf.Module):\n","  def __init__(self, reserved_tokens, vocab_path):\n","    self.tokenizer = text.BertTokenizer(vocab_path, lower_case=True)\n","    self._reserved_tokens = reserved_tokens\n","    self._vocab_path = tf.saved_model.Asset(vocab_path)\n","\n","    vocab = pathlib.Path(vocab_path).read_text().splitlines()\n","    self.vocab = tf.Variable(vocab)\n","\n","    ## Create the signatures for export:   \n","\n","    # Include a tokenize signature for a batch of strings. \n","    self.tokenize.get_concrete_function(\n","        tf.TensorSpec(shape=[None], dtype=tf.string))\n","\n","    # Include `detokenize` and `lookup` signatures for:\n","    #   * `Tensors` with shapes [tokens] and [batch, tokens]\n","    #   * `RaggedTensors` with shape [batch, tokens]\n","    self.detokenize.get_concrete_function(\n","        tf.TensorSpec(shape=[None, None], dtype=tf.int64))\n","    self.detokenize.get_concrete_function(\n","          tf.RaggedTensorSpec(shape=[None, None], dtype=tf.int64))\n","\n","    self.lookup.get_concrete_function(\n","        tf.TensorSpec(shape=[None, None], dtype=tf.int64))\n","    self.lookup.get_concrete_function(\n","          tf.RaggedTensorSpec(shape=[None, None], dtype=tf.int64))\n","\n","    # These `get_*` methods take no arguments\n","    self.get_vocab_size.get_concrete_function()\n","    self.get_vocab_path.get_concrete_function()\n","    self.get_reserved_tokens.get_concrete_function()\n","\n","  @tf.function\n","  def tokenize(self, strings):\n","    enc = self.tokenizer.tokenize(strings)\n","    # Merge the `word` and `word-piece` axes.\n","    enc = enc.merge_dims(-2,-1)\n","    enc = add_start_end(enc)\n","    return enc\n","\n","  @tf.function\n","  def detokenize(self, tokenized):\n","    words = self.tokenizer.detokenize(tokenized)\n","    return cleanup_text(self._reserved_tokens, words)\n","\n","  @tf.function\n","  def lookup(self, token_ids):\n","    return tf.gather(self.vocab, token_ids)\n","\n","  @tf.function\n","  def get_vocab_size(self):\n","    return tf.shape(self.vocab)[0]\n","\n","  @tf.function\n","  def get_vocab_path(self):\n","    return self._vocab_path\n","\n","  @tf.function\n","  def get_reserved_tokens(self):\n","    return tf.constant(self._reserved_tokens)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GubNU_HK_dX7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1624123276943,"user_tz":360,"elapsed":3271,"user":{"displayName":"Tim Sullivan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgDOmSXGsobJcA9UOtHRwU-sWjSZffSAqqvuM2IHQ=s64","userId":"11783081476684672772"}},"outputId":"5440e25f-fba1-4d6a-8e89-34f6f2909705"},"source":["tokenizers = tf.Module()\n","tokenizers.ru = CustomTokenizer(reserved_tokens, 'ru_vocab.txt')\n","tokenizers.en = CustomTokenizer(reserved_tokens, 'en_vocab.txt')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/array_ops.py:5049: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n","Instructions for updating:\n","The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/array_ops.py:5049: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n","Instructions for updating:\n","The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"Q_bxYzmM_il8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1624123278894,"user_tz":360,"elapsed":1959,"user":{"displayName":"Tim Sullivan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgDOmSXGsobJcA9UOtHRwU-sWjSZffSAqqvuM2IHQ=s64","userId":"11783081476684672772"}},"outputId":"3138d17d-4572-4901-8621-d15b14b1feca"},"source":["model_name = 'ted_hrlr_translate_ru_en_converter'\n","tf.saved_model.save(tokenizers, model_name);"],"execution_count":null,"outputs":[{"output_type":"stream","text":["INFO:tensorflow:Assets written to: ted_hrlr_translate_ru_en_converter/assets\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Assets written to: ted_hrlr_translate_ru_en_converter/assets\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"cmLu95UP_oLK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1624123280146,"user_tz":360,"elapsed":1256,"user":{"displayName":"Tim Sullivan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgDOmSXGsobJcA9UOtHRwU-sWjSZffSAqqvuM2IHQ=s64","userId":"11783081476684672772"}},"outputId":"7bfb0cb5-1199-4609-a175-99c0480a9b72"},"source":["reloaded_tokenizers = tf.saved_model.load(model_name)\n","reloaded_tokenizers.en.get_vocab_size().numpy()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["7796"]},"metadata":{"tags":[]},"execution_count":38}]},{"cell_type":"code","metadata":{"id":"AN7GpSbX_rhe","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1624123280931,"user_tz":360,"elapsed":789,"user":{"displayName":"Tim Sullivan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgDOmSXGsobJcA9UOtHRwU-sWjSZffSAqqvuM2IHQ=s64","userId":"11783081476684672772"}},"outputId":"b2627f89-dd28-4788-f83a-ac1448e8d831"},"source":["tokens = reloaded_tokenizers.en.tokenize(['Hello TensorFlow!'])\n","print(tokens.numpy())\n","\n","round_trip = reloaded_tokenizers.en.detokenize(tokens)\n","print(round_trip.numpy()[0].decode('utf-8'))\n","\n","print(tokenizers.en.lookup(tokens))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[[   2 3372 2214  691  952 2669    4    3]]\n","hello tensorflow !\n","<tf.RaggedTensor [[b'[START]', b'hello', b'tens', b'##or', b'##f', b'##low', b'!', b'[END]']]>\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"icVKXWT4ymXS"},"source":["Make the pipeline"]},{"cell_type":"code","metadata":{"id":"Dm7Eby5PyjWZ"},"source":["def tokenize_pairs(ru, en):\n","    ru = tokenizers.ru.tokenize(ru)\n","    # Convert from ragged to dense, padding with zeros.\n","    ru = ru.to_tensor()\n","\n","    en = tokenizers.en.tokenize(en)\n","    # Convert from ragged to dense, padding with zeros.\n","    en = en.to_tensor()\n","    return ru, en"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3PzlFybPyrRD"},"source":["BUFFER_SIZE = 20000\n","BATCH_SIZE = 32 # reduced for memory"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"40Y0Y2qOyrzw"},"source":["def make_batches(ds):\n","  return (\n","      ds\n","      .cache()\n","      .shuffle(BUFFER_SIZE)\n","      .batch(BATCH_SIZE)\n","      .map(tokenize_pairs, num_parallel_calls=tf.data.AUTOTUNE)\n","      .prefetch(tf.data.AUTOTUNE))\n","\n","\n","train_batches = make_batches(train_examples)\n","val_batches = make_batches(val_examples)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XRZUv6RPzklZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1624123283112,"user_tz":360,"elapsed":1508,"user":{"displayName":"Tim Sullivan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgDOmSXGsobJcA9UOtHRwU-sWjSZffSAqqvuM2IHQ=s64","userId":"11783081476684672772"}},"outputId":"dbfbdebb-5022-4da7-8c0e-ff108fb7315f"},"source":["list(train_batches.take(2).as_numpy_iterator())"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[(array([[   2,   70, 3322, ...,    0,    0,    0],\n","         [   2, 1017,  148, ...,    0,    0,    0],\n","         [   2,  107,  181, ...,    0,    0,    0],\n","         ...,\n","         [   2,  469,   88, ...,    0,    0,    0],\n","         [   2,   79,  368, ...,    0,    0,    0],\n","         [   2,   10,  678, ...,    0,    0,    0]]),\n","  array([[   2,   38,   38, ...,    0,    0,    0],\n","         [   2,   85,  108, ...,    0,    0,    0],\n","         [   2,   47, 6750, ...,    0,    0,    0],\n","         ...,\n","         [   2,  102,   84, ...,    0,    0,    0],\n","         [   2,   85,  108, ...,    0,    0,    0],\n","         [   2,   10,  365, ...,    0,    0,    0]])),\n"," (array([[   2,   86, 2857, ...,    0,    0,    0],\n","         [   2,  163,   14, ...,    0,    0,    0],\n","         [   2,  333,  131, ...,  548,   16,    3],\n","         ...,\n","         [   2,   40,   40, ...,    0,    0,    0],\n","         [   2,  471,  231, ...,    0,    0,    0],\n","         [   2,  435,   14, ...,    0,    0,    0]]),\n","  array([[   2, 3164,   89, ...,    0,    0,    0],\n","         [   2,   89,   94, ...,    0,    0,    0],\n","         [   2,   90,    9, ..., 1061,   16,    3],\n","         ...,\n","         [   2,   38,   38, ...,    0,    0,    0],\n","         [   2,  108,   99, ...,    0,    0,    0],\n","         [   2,   92,  138, ...,    0,    0,    0]]))]"]},"metadata":{"tags":[]},"execution_count":43}]},{"cell_type":"markdown","metadata":{"id":"GQlcI-Y91k3P"},"source":["## Position encoding\n","\n","We ditched the recurrent structure due to the long gradient problem, but the position of words in the sequence is still valuable.\n","\n","The embedding groups words by meaning, and adding the positional encoding adds absolute and relative position information.\n","\n","$$ PE_{(pos, 2i)}=sin(pos/10000^{2i/d_{model}})$$\n","$$ PE_{(pos, 2i+1)}=cos(pos/10000^{2i/d_{model}})$$"]},{"cell_type":"code","metadata":{"id":"QMNqzpIF1NBx"},"source":["def get_angles(pos, i , d_model):\n","    angle_rates = 1 / np.power(10000, (2*(i//2)) / np.float32(d_model))\n","    return pos * angle_rates\n","\n","def positional_encoding(position, d_model):\n","    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n","                            np.arange(d_model)[np.newaxis,:],\n","                            d_model)\n","    # sin to even i\n","    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n","\n","    # cos to odd indices 2i+1\n","    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n","\n","    pos_encoding = angle_rads[np.newaxis, ...]\n","\n","    return tf.cast(pos_encoding, dtype=tf.float32)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":463},"id":"84r-xeHW3IH_","executionInfo":{"status":"ok","timestamp":1624126543012,"user_tz":360,"elapsed":17,"user":{"displayName":"Tim Sullivan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgDOmSXGsobJcA9UOtHRwU-sWjSZffSAqqvuM2IHQ=s64","userId":"11783081476684672772"}},"outputId":"1e2bdf5b-22ba-44b8-935e-c2298d71f84a"},"source":["n, d = 25, 512\n","pos_encoding = positional_encoding(n, d)\n","print(pos_encoding.shape)\n","pos_encoding = pos_encoding[0]\n","\n","# Juggle the dimensions for the plot\n","pos_encoding = tf.reshape(pos_encoding, (n, d//2, 2))\n","pos_encoding = tf.transpose(pos_encoding, (2, 1, 0))\n","pos_encoding = tf.reshape(pos_encoding, (d, n))\n","\n","import matplotlib.pyplot as plt\n","plt.figure(figsize=(10,7))\n","plt.pcolormesh(pos_encoding[:20,:], cmap='RdBu')\n","plt.ylabel('Depth')\n","plt.xlabel('Position')\n","plt.colorbar()\n","plt.show()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(1, 25, 512)\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 720x504 with 2 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"PPn5VkRy2ZOv"},"source":["## Masking\n","\n","Two types of masking are required. One is for the tokenized inputs which are padding with zeros. This is the padding mask. The other is to mask other tokens in the sequence that the model is trying to learn. This is the look ahead mask."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iVOkOKn178UL","executionInfo":{"status":"ok","timestamp":1623885428727,"user_tz":360,"elapsed":39,"user":{"displayName":"Tim Sullivan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgDOmSXGsobJcA9UOtHRwU-sWjSZffSAqqvuM2IHQ=s64","userId":"11783081476684672772"}},"outputId":"6813c207-4bbf-44f4-c223-ce731706ccfc"},"source":["def create_padding_mask(seq):\n","    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n","\n","    # adding dimensions for attention logits compabitility\n","    return seq[:, tf.newaxis, tf.newaxis, :] #(batch_size, 1, 1, seq_len)\n","\n","x = tf.constant([[7, 6, 0, 0, 1], [1, 2, 3, 0, 0], [0, 0, 0, 4, 5]])\n","create_padding_mask(x)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(3, 1, 1, 5), dtype=float32, numpy=\n","array([[[[0., 0., 1., 1., 0.]]],\n","\n","\n","       [[[0., 0., 0., 1., 1.]]],\n","\n","\n","       [[[1., 1., 1., 0., 0.]]]], dtype=float32)>"]},"metadata":{"tags":[]},"execution_count":30}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UP5HctVp9Um3","executionInfo":{"status":"ok","timestamp":1623885428728,"user_tz":360,"elapsed":30,"user":{"displayName":"Tim Sullivan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgDOmSXGsobJcA9UOtHRwU-sWjSZffSAqqvuM2IHQ=s64","userId":"11783081476684672772"}},"outputId":"e1e59428-4211-41d4-f88b-7a11287ce1ef"},"source":["def create_look_ahead_mask(size):\n","    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n","    return mask  # (seq_len, seq_len)\n","\n","x = tf.random.uniform((1, 3))\n","temp = create_look_ahead_mask(x.shape[1])\n","temp"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(3, 3), dtype=float32, numpy=\n","array([[0., 1., 1.],\n","       [0., 0., 1.],\n","       [0., 0., 0.]], dtype=float32)>"]},"metadata":{"tags":[]},"execution_count":31}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8LhbGTM4-SA1","executionInfo":{"status":"ok","timestamp":1623885428729,"user_tz":360,"elapsed":23,"user":{"displayName":"Tim Sullivan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgDOmSXGsobJcA9UOtHRwU-sWjSZffSAqqvuM2IHQ=s64","userId":"11783081476684672772"}},"outputId":"f4a27ed0-e33a-4732-fb16-66d798026e04"},"source":["a = tf.ones((3,3))\n","tf.linalg.band_part(a, 0, 0)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(3, 3), dtype=float32, numpy=\n","array([[1., 0., 0.],\n","       [0., 1., 0.],\n","       [0., 0., 1.]], dtype=float32)>"]},"metadata":{"tags":[]},"execution_count":32}]},{"cell_type":"markdown","metadata":{"id":"SmfNEG-_1j0r"},"source":["## Embedding\n","- Learned embeddings convert tokens to vectors of dimension $d_\\text{model}$"]},{"cell_type":"markdown","metadata":{"id":"EwkWYTQ1A0jg"},"source":["# Model"]},{"cell_type":"markdown","metadata":{"id":"BRRLenCxtfMw"},"source":["## Encoder\n","- 6 layers\n","- 2 sub-layers per layer\n","    - Embedd\n","    - Position encoding\n","    - Multi-head self attention\n","    - FC Feed-forward\n","    - Residual connections around each sub-layer\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"LJBow0-_1RiP"},"source":["# The encoder and decoder also contain a feed forward section with two dense\n","# layers and relu activation between\n","def point_wise_feed_forward_network(d_model, d_ff):\n","    return tf.keras.Sequential([layers.Dense(d_ff, activation='relu'),\n","                                layers.Dense(d_model)])\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_QfqTCVrj9FX"},"source":["class EncoderLayer(layers.Layer):\n","    def __init__(self, d_model, num_heads, d_ff, dropout_rate=0.1):\n","        super(EncoderLayer, self).__init__()\n","\n","        self.mha = MultiHeadAttention(d_model, num_heads)\n","        self.ffn = point_wise_feed_forward_network(d_model, d_ff)\n","\n","        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n","        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n","\n","        self.dropout1 = layers.Dropout(dropout_rate)\n","        self.dropout2 = layers.Dropout(dropout_rate)\n","\n","    def call(self, x, training, mask):\n","        attn_out, _ = self.mha(x, x, x, mask) # (batch_size, input_seq_len, d_model)\n","        attn_out = self.dropout1(attn_out, training=training)\n","        out1 = self.layernorm1(x + attn_out)\n","\n","        ff_out = self.ffn(out1)\n","        ff_out = self.dropout2(ff_out, training=training)\n","        out2 = self.layernorm2(out1 + ff_out) # (batch_size, input_seq_len, d_model)\n","\n","        return out2\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DgJvMyUtLAdu","executionInfo":{"status":"ok","timestamp":1623885428889,"user_tz":360,"elapsed":11,"user":{"displayName":"Tim Sullivan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgDOmSXGsobJcA9UOtHRwU-sWjSZffSAqqvuM2IHQ=s64","userId":"11783081476684672772"}},"outputId":"d86c322f-4159-4e58-faad-f73ed0ea105e"},"source":["tf.random.set_seed(42)\n","sample_encoder_layer = EncoderLayer(512, 8, 2048)\n","\n","sample_encoder_layer_output = sample_encoder_layer(\n","    tf.random.uniform((64, 43, 512)), False, None)\n","\n","sample_encoder_layer_output.shape  # (batch_size, input_seq_len, d_model)\n"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["TensorShape([64, 43, 512])"]},"metadata":{"tags":[]},"execution_count":35}]},{"cell_type":"code","metadata":{"id":"vDuyHtLXj9AH"},"source":["class Encoder(layers.Layer):\n","    def __init__(self, num_layers, d_model, num_heads, d_ff, input_vocab_size,\n","                 maximum_position_encoding, dropout_rate=0.1):\n","        super(Encoder, self).__init__()\n","\n","        self.d_model = d_model\n","        self.num_layers = num_layers\n","\n","        self.embedding = layers.Embedding(input_vocab_size, d_model)\n","        self.pos_encoding = positional_encoding(maximum_position_encoding,\n","                                                self.d_model)\n","        \n","        self.enc_layers = [EncoderLayer(d_model, num_heads, d_ff, dropout_rate)\n","                            for i in range(num_layers)]\n","        self.dropout = layers.Dropout(dropout_rate)\n","\n","    def call(self, x, training, mask):\n","        \n","        seq_len = tf.shape(x)[1]\n","        \n","        # prep inputs with embedding and pos encoding\n","        x = self.embedding(x) # (batch_size, input_seq_len, d_model)\n","        # embedding layer weights are multiplied by sqrt(d_model) sec 3.4\n","        x *= tf.sqrt(tf.cast(self.d_model, tf.float32))\n","        x += self.pos_encoding[:, :seq_len, :]\n","        \n","\n","        x = self.dropout(x, training=training)\n","        # now go through all the encoding layers\n","        for i in range(self.num_layers):\n","            x = self.enc_layers[i](x, training, mask)\n","\n","        return x # (batch_size, input_seq_len, d_model)\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7P_7olzrj85P","executionInfo":{"status":"ok","timestamp":1623885429033,"user_tz":360,"elapsed":152,"user":{"displayName":"Tim Sullivan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgDOmSXGsobJcA9UOtHRwU-sWjSZffSAqqvuM2IHQ=s64","userId":"11783081476684672772"}},"outputId":"8814891d-7df9-4b76-e1ac-c689be49ef1d"},"source":["tf.random.set_seed(42)\n","sample_encoder = Encoder(num_layers=2, d_model=512, num_heads=8,\n","                         d_ff=2048, input_vocab_size=8500,\n","                         maximum_position_encoding=10000)\n","temp_input = tf.random.uniform((64, 62), dtype=tf.int64, minval=0, maxval=200)\n","sample_encoder_output = sample_encoder(temp_input, training=False, mask=None)\n","\n","print(sample_encoder_output.shape)  # (batch_size, input_seq_len, d_model)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(64, 62, 512)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"-F8ZPl9w0g02"},"source":["## Decoder\n","- 6 layers\n","- Same 2 layers as Encoder but with a third layer between\n","    - Middle layer performs multi-head attention on output from Encoder\n","    - Same residual connections"]},{"cell_type":"code","metadata":{"id":"tuuFQ_GHmXGL"},"source":["class DecoderLayer(layers.Layer):\n","    def __init__(self, d_model, num_heads, d_ff, dropout_rate=0.1):\n","        super(DecoderLayer, self).__init__()\n","\n","        self.mmha = MultiHeadAttention(d_model, num_heads)\n","        self.mha = MultiHeadAttention(d_model, num_heads)\n","        self.ffn = point_wise_feed_forward_network(d_model, d_ff)\n","\n","        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n","        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n","        self.layernorm3 = layers.LayerNormalization(epsilon=1e-6)\n","\n","        self.dropout1 = layers.Dropout(dropout_rate)\n","        self.dropout2 = layers.Dropout(dropout_rate)\n","        self.dropout3 = layers.Dropout(dropout_rate)\n","\n","    def call(self, x, encoder_out, training, look_ahead_mask, padding_mask):\n","        mask_attn_out, attn_weights_block1 = self.mmha(x, x, x, look_ahead_mask) # (batch_size, target_seq_len, d_model)\n","        mask_attn_out = self.dropout1(mask_attn_out, training=training)\n","        out1 = self.layernorm1(mask_attn_out + x)\n","\n","        attn_out, attn_weights_block2 = self.mha(encoder_out, encoder_out, \n","                                                 out1, padding_mask)\n","        attn_out = self.dropout2(attn_out, training=training)\n","        out2 = self.layernorm2(attn_out + out1)\n","\n","        ff_out = self.ffn(out2)\n","        ff_out = self.dropout3(ff_out, training=training)\n","        out3 = self.layernorm3(ff_out + out2) # (batch_size, target_seq_len, d_model)\n","\n","        return out3, attn_weights_block1, attn_weights_block2"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"F4dTu2D5mXNJ"},"source":["class Decoder(layers.Layer):\n","    def __init__(self, num_layers, d_model, num_heads, d_ff, target_vocab_size,\n","                 maximum_position_encoding, dropout_rate=0.1):\n","        super(Decoder, self).__init__()\n","\n","        self.d_model = d_model\n","        self.num_layers = num_layers\n","\n","        self.embedding = layers.Embedding(target_vocab_size, d_model)\n","        self.pos_encoding = positional_encoding(maximum_position_encoding,\n","                                                d_model)\n","        \n","        self.dec_layers = [DecoderLayer(d_model, num_heads, d_ff, dropout_rate)\n","                            for i in range(num_layers)]\n","        self.dropout = layers.Dropout(dropout_rate)\n","\n","    def call(self, x, enc_output, training,  \n","             look_ahead_mask, padding_mask):\n","        \n","        seq_len = tf.shape(x)[1]\n","        attn_weights = {}\n","        \n","        # prep inputs with embedding and pos encoding\n","        x = self.embedding(x) # (batch_size, input_seq_len, d_model)\n","        # embedding layer weights are multiplied by sqrt(d_model) sec 3.4\n","        x *= tf.sqrt(tf.cast(self.d_model, tf.float32))\n","        x += self.pos_encoding[:, :seq_len, :]\n","\n","        x = self.dropout(x, training=training)\n","\n","        # now go through all the encoding layers\n","        for i in range(self.num_layers):\n","            x, block1, block2 = self.dec_layers[i](x, enc_output, training, \n","                                                   look_ahead_mask,padding_mask)\n","            attn_weights[f'decoder_layer{i+1}_block1'] = block1\n","            attn_weights[f'decoder_layer{i+1}_block2'] = block2\n","\n","        return x, attn_weights # (batch_size, input_seq_len, d_model)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yMbNCs5fmXUJ","executionInfo":{"status":"ok","timestamp":1623885429218,"user_tz":360,"elapsed":192,"user":{"displayName":"Tim Sullivan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgDOmSXGsobJcA9UOtHRwU-sWjSZffSAqqvuM2IHQ=s64","userId":"11783081476684672772"}},"outputId":"189ea5c3-123e-4a90-cfcd-66cb825345fc"},"source":["tf.random.set_seed(42)\n","sample_decoder = Decoder(num_layers=2, d_model=512, num_heads=8,\n","                         d_ff=2048, target_vocab_size=8000,\n","                         maximum_position_encoding=5000)\n","temp_input = tf.random.uniform((64, 26), dtype=tf.int64, minval=0, maxval=200)\n","\n","output, attn = sample_decoder(temp_input,\n","                              enc_output=sample_encoder_output,\n","                              training=False,\n","                              look_ahead_mask=None,\n","                              padding_mask=None)\n","\n","output.shape, attn['decoder_layer2_block2'].shape\n"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(TensorShape([64, 26, 512]), TensorShape([64, 8, 26, 62]))"]},"metadata":{"tags":[]},"execution_count":40}]},{"cell_type":"markdown","metadata":{"id":"ZsDVHDjNmXiN"},"source":["# Transformer\n","Finally construct the full transformer."]},{"cell_type":"code","metadata":{"id":"36gPZPqGeCFH"},"source":["class Transformer(tf.keras.Model):\n","    def __init__(self, num_layers, d_model, num_heads, d_ff, input_vocab_size,\n","                 target_vocab_size, pe_input, pe_target, dropout_rate=0.1):\n","        super(Transformer, self).__init__()\n","    \n","        self.encoder = Encoder(num_layers, d_model, num_heads, d_ff, \n","                            input_vocab_size, pe_input, dropout_rate)\n","        \n","        self.decoder = Decoder(num_layers, d_model, num_heads, d_ff, \n","                            input_vocab_size, pe_target, dropout_rate)\n","        \n","        self.final_layer = layers.Dense(target_vocab_size)\n","\n","    def call(self, inp, tar, training, enc_padding_mask, look_ahead_mask,\n","             dec_padding_mask):\n","        # (batch_size, inp_seq_len, d_model)\n","        enc_output = self.encoder(inp, training, enc_padding_mask)\n","\n","        dec_output, attention_weights  = self.decoder(tar, enc_output, training, \n","                                                look_ahead_mask,dec_padding_mask)\n","        \n","        # (batch_size, tar_seq_len, target_vocab_size)\n","        final_output = self.final_layer(dec_output)\n","\n","        return final_output, attention_weights\n","\n","\n"],"execution_count":nu