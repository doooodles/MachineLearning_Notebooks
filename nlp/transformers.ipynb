{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"transformers.ipynb","provenance":[],"collapsed_sections":["swDUGQDm7dfe","SmfNEG-_1j0r"],"mount_file_id":"1dAycfA3ldhHktD80zKUNXalR0QuSCadC","authorship_tag":"ABX9TyML7lHJO6R47ZXtQY3BuaWC"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"swDUGQDm7dfe"},"source":["# Transformers\n","\n","Paper: [Attention is all you need](https://arxiv.org/abs/1706.03762)\n","\n","Resources:\n","- [The Illustrated Transformer\n","](https://jalammar.github.io/illustrated-transformer/)\n","- [Visualizing A Neural Machine Translation Model (Mechanics of Seq2seq Models With Attention)](https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/)\n","- [TF Docs: Transformer model for language understanding\n","](https://www.tensorflow.org/text/tutorials/transformer)\n","\n","#### Summary\n","- Network based only on attention, without recurrence or convolutions.\n","- Addresses long sequence dependence problem of recurrent models with single encoder hidden state.\n","- Position information is included separately \n","\n","### Approach\n","\n","- Until done:\n","    1. Read the paper\n","    2. Try to implement until stuck\n","    3. Check resources\n","    4. Go to 1.\n","The end product will look a lot like the TF example, *but* I'll learn a lot along the way.\n","\n","### Results\n","- Tried both Portugese to English from the TF docs and Russian to English.\n","- The Russian to English did not perform as well given the time allocated for training and may need additional hyperparameter tuning.\n","\n","[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tims457/ml_notebooks/blob/main/nlp/transformers.ipynb)\n"]},{"cell_type":"code","metadata":{"id":"CGY6wp3dAJOq"},"source":["import tensorflow as tf\n","import numpy as np\n","import tensorflow_datasets as tfds\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8Zg0LYfqACrO"},"source":["# Attention model\n","\n","### Notation\n","- $Q$ = Query\n","- $K$ = Key, dimension $d_k$ = 64\n","- $V$ = Value, dimension $d_v$ = 64\n","- $n$ = input sequence length\n","- $m$ = output sequence length\n","- $d_\\text{model}$ = model dimension = 512\n","- $d_{ff}$ = inner layer dimension = 2048\n","- $h$ = heads\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Yt2vmmu7Ntjy"},"source":["## Attention\n","\n","### Basics\n","- Deals with problem of relationships between elements of a long sequence experienced by recurrent models.\n","- Pass all hidden states (all encoder output) to decoder.\n","- Decoder scores each of the encoder hidden states, applies a softmax to the score, then multiplies the score times the hidden state from the encoder. This increases the value of states that scored higher.\n","- Rather than having a single hidden state from a recurrent sequence, the decoder has *all* the hidden states and a score (or attention) value for each of them to weight their importance.\n","- Self-attention means the network is learning to associate the relationships between input elements. \n","\n","<img src=\"https://www.researchgate.net/publication/333078019/figure/fig1/AS:758304078839808@1557805189409/left-Scaled-Dot-Product-Attention-right-Multi-Head-Attention.png\" height=300 />"]},{"cell_type":"markdown","metadata":{"id":"N9EJUz9cK5fq"},"source":["## Multi-head attention\n","- Query, Key, Value (Q,K,V) for each of the encoder input vector created by the embedding.\n","- Attention value is the softmax of Query x Key scaled by the dimension $d_k$ then multiplied by Value\n","\n","$$\\text{Attention(Q,K,V)} = \\text{softmax}_k(\\frac{QK^T}{\\sqrt{d_k}})V$$\n","\n","- Multiple heads - number of attention layers running in parallel\n","- By starting with multiple heads (8 in this case) the encoder and decoder are really multiple encoders and decoders operating in parallel. \n","\n"]},{"cell_type":"markdown","metadata":{"id":"19f35FemNUHx"},"source":[""]},{"cell_type":"code","metadata":{"id":"YdGIapvC0VNx"},"source":["import tensorflow as tf\n","from tensorflow.keras import layers\n","\n","# Attention\n","# sec 3.2.1\n","def scaled_dot_product_attention(q, k, v, mask):\n","    # get dimensions of the input, cast from tensor to float\n","    d_k = tf.cast(tf.shape(k)[-1], tf.float32)\n","    \n","    # compute queries x keys and scale by dimension\n","    attention_logits = tf.matmul(q, k, transpose_b=True)\n","    \n","    scaled_attention_logits = attention_logits / tf.math.sqrt(d_k)\n","    # print(f\"scaled attention shape {scaled_attention_logits.shape}\")\n","\n","    # apply decoder mask\n","    if mask is not None:\n","        scaled_attention_logits += (mask * -1e9)\n","\n","    # normalize all scores\n","    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n","    # print(f\"attention shape {attention_weights.shape}\")\n","\n","    # times value\n","    output = tf.matmul(attention_weights, v)\n","    # print(f\"output shape {output.shape}\")\n","\n","    return output, attention_weights"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fq0feSF1aa-o","executionInfo":{"status":"ok","timestamp":1624123270577,"user_tz":360,"elapsed":63,"user":{"displayName":"Tim Sullivan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgDOmSXGsobJcA9UOtHRwU-sWjSZffSAqqvuM2IHQ=s64","userId":"11783081476684672772"}},"outputId":"e676b2d6-cfd4-4eb8-ba2e-d9ee1e5d1ae8"},"source":["def print_out(q, k, v):\n","    temp_out, temp_attn = scaled_dot_product_attention(q, k, v, None)\n","    print('Attention weights are:')\n","    print(np.round(temp_attn, decimals=2))\n","    print('Output is:')\n","    print(np.round(temp_out, decimals=2))\n","\n","temp_k = tf.constant([[10, 0, 0],\n","                      [0, 10, 0],\n","                      [0, 0, 10],\n","                      [0, 0, 10]], dtype=tf.float32)  # (4, 3)\n","\n","temp_v = tf.constant([[1, 0],\n","                      [10, 0],\n","                      [100, 5],\n","                      [1000, 6]], dtype=tf.float32)  # (4, 2)\n","\n","\n","# The dot product attention is selecting the key that aligns with the \n","# query and then returning the associated value.\n","temp_q = tf.constant([[0, 10, 0]], dtype=tf.float32)  # (1, 3)\n","print_out(temp_q, temp_k, temp_v)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Attention weights are:\n","[[0. 1. 0. 0.]]\n","Output is:\n","[[10.  0.]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"B1fsCrjh02cu"},"source":["Use the `scaled_dot_product_attention` layer to get a handle on how the model is selecting query-key pairs and computing their value.\n"]},{"cell_type":"code","metadata":{"id":"fcjJuOdxAHt7"},"source":["# sec 3.2.2\n","class MultiHeadAttention(tf.keras.layers.Layer):\n","    def __init__(self, d_model, num_heads):\n","        super(MultiHeadAttention, self).__init__()\n","        self.num_heads = num_heads\n","        self.d_model = d_model\n","\n","        assert self.d_model % self.num_heads == 0 \n","\n","        self.depth = self.d_model  // num_heads\n","\n","        self.wq = layers.Dense(d_model)\n","        self.wk = layers.Dense(d_model)\n","        self.wv = layers.Dense(d_model)\n","\n","        self.dense = layers.Dense(d_model)\n","\n","    def split_heads(self, x, batch_size):\n","        \"\"\"\n","        The inputs need to be reshaped in order to be fed into the attention portion.\n","        The model dimension d_k get split into heads x depth.\n","        Then transposed to (batch_size, num_heads, seq_len, depth)\n","\n","        \"\"\"\n","        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n","        return tf.transpose(x, perm=[0, 2, 1, 3])\n","\n","    # forward computation\n","    def call(self, v, k, q, mask):\n","        batch_size = tf.shape(q)[0]\n","\n","        q = self.wq(q) # (batch_size, seq_len, d_model)\n","        k = self.wk(k)\n","        v = self.wv(v)\n","\n","        q = self.split_heads(q, batch_size) # (batch_size, num_heads, seq_len_q, depth)\n","        k = self.split_heads(k, batch_size)\n","        v = self.split_heads(v, batch_size)\n","\n","        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n","        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n","        scaled_attention, attention_weights = scaled_dot_product_attention(q, k, v, mask)\n","\n","        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n","\n","        concat_attention = tf.reshape(scaled_attention,\n","                                    (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n","\n","        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n","\n","        return output, attention_weights\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1ZXWw8OQrT2T","executionInfo":{"status":"ok","timestamp":1624123270579,"user_tz":360,"elapsed":61,"user":{"displayName":"Tim Sullivan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgDOmSXGsobJcA9UOtHRwU-sWjSZffSAqqvuM2IHQ=s64","userId":"11783081476684672772"}},"outputId":"806beebd-073b-493c-cb10-f502967fa32c"},"source":["tf.random.set_seed(42)\n","mha = MultiHeadAttention(512, 8)\n","\n","x = tf.ones((1,60, 512))\n","out, attn = mha(x,k=x,q=x, mask=None)\n","out.shape, attn.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(TensorShape([1, 60, 512]), TensorShape([1, 8, 60, 60]))"]},"metadata":{"tags":[]},"execution_count":23}]},{"cell_type":"markdown","metadata":{"id":"txzzj49L_30r"},"source":["# Data\n","- Used English-German and English-French dataset. 4.5 million and 36 million sentences respectively\n","- 37000 word English-German vocab\n","- 32000 word English-French vocab\n","- Instead let's use the TF dataset for Russian to English"]},{"cell_type":"code","metadata":{"id":"xcFqTEr83YFW"},"source":["examples, metadata = tfds.load('ted_hrlr_translate/ru_to_en', with_info=True,\n","                               as_supervised=True)\n","train_examples, val_examples = examples['train'], examples['validation']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zkFVV1l956It","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1624123270915,"user_tz":360,"elapsed":19,"user":{"displayName":"Tim Sullivan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgDOmSXGsobJcA9UOtHRwU-sWjSZffSAqqvuM2IHQ=s64","userId":"11783081476684672772"}},"outputId":"9c3251d9-7515-4921-f32b-acf0fd8f3ec7"},"source":["#print  examples\n","for ru, en in train_examples.take(1):\n","  print(\"Russian: \", ru.numpy().decode('utf-8'))\n","  print(\"English:   \", en.numpy().decode('utf-8'))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Russian:  к : успех , перемены возможны только с оружием в руках .\n","English:    c : success , the change is only coming through the barrel of the gun .\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"7M6umRAu6e9t"},"source":["### Tokenization\n","- There's no tokenizer for the `ted hrlr` Russian to English set that I can find so I have to make one.\n","- The Google docs example for attention said they used a sub-word version built with Bert.\n","- I'll just use the example here https://www.tensorflow.org/text/guide/subwords_tokenizer"]},{"cell_type":"code","metadata":{"id":"TioATfFL8tdX"},"source":["!pip install -q -U tensorflow-text"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WFL54Yfg614k"},"source":["\n","import tensorflow_text as text\n","from tensorflow_text.tools.wordpiece_vocab import bert_vocab_from_dataset as bert_vocab\n","\n","train_en = train_examples.map(lambda ru, en: en)\n","train_ru = train_examples.map(lambda ru, en: ru)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Fjtfi7DkPM09"},"source":["This section will generate the English and Russain vocabulary, but it takes a long time so use the included `txt` files."]},{"cell_type":"code","metadata":{"id":"wFxzKMJi8xbD"},"source":["bert_tokenizer_params=dict(lower_case=True)\n","reserved_tokens=[\"[PAD]\", \"[UNK]\", \"[START]\", \"[END]\"]\n","\n","\n","# bert_vocab_args = dict(\n","#     # The target vocabulary size\n","#     vocab_size = 8000,\n","#     # Reserved tokens that must be included in the vocabulary\n","#     reserved_tokens=reserved_tokens,\n","#     # Arguments for `text.BertTokenizer`\n","#     bert_tokenizer_params=bert_tokenizer_params,\n","#     # Arguments for `wordpiece_vocab.wordpiece_tokenizer_learner_lib.learn`\n","#     learn_params={},\n","# )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"plzF-nFw9Bps"},"source":["# %%time\n","# en_vocab = bert_vocab.bert_vocab_from_dataset(\n","#     train_en.batch(1000).prefetch(2),\n","#     **bert_vocab_args\n","# )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"t2Hkjven9CSD"},"source":["# print(en_vocab[:10])\n","# print(en_vocab[100:110])\n","# print(en_vocab[1000:1010])\n","# print(en_vocab[-10:])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QDzCJsx-9CeJ"},"source":["# %%time\n","# ru_vocab = bert_vocab.bert_vocab_from_dataset(\n","#     train_ru.batch(1000).prefetch(2),\n","#     **bert_vocab_args\n","# )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NobcBT1P9CmQ"},"source":["# print(ru_vocab[:10])\n","# print(ru_vocab[100:110])\n","# print(ru_vocab[1000:1010])\n","# print(ru_vocab[-10:])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jSajpI4T9CuF"},"source":["# def write_vocab_file(filepath, vocab):\n","#   with open(filepath, 'w') as f:\n","#     for token in vocab:\n","#       print(token, file=f)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gYVqjS_b9C1J"},"source":["# write_vocab_file('en_vocab.txt', en_vocab)\n","# write_vocab_file('ru_vocab.txt', ru_vocab)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"n0pWFxlt9DDS"},"source":["# copying this module from the TF docs.\n","\n","import pathlib\n","import re\n","\n","START = tf.argmax(tf.constant(reserved_tokens) == \"[START]\")\n","END = tf.argmax(tf.constant(reserved_tokens) == \"[END]\")\n","\n","def cleanup_text(reserved_tokens, token_txt):\n","  # Drop the reserved tokens, except for \"[UNK]\".\n","  bad_tokens = [re.escape(tok) for tok in reserved_tokens if tok != \"[UNK]\"]\n","  bad_token_re = \"|\".join(bad_tokens)\n","\n","  bad_cells = tf.strings.regex_full_match(token_txt, bad_token_re)\n","  result = tf.ragged.boolean_mask(token_txt, ~bad_cells)\n","\n","  # Join them into strings.\n","  result = tf.strings.reduce_join(result, separator=' ', axis=-1)\n","\n","  return result\n","\n","def add_start_end(ragged):\n","  count = ragged.bounding_shape()[0]\n","  starts = tf.fill([count,1], START)\n","  ends = tf.fill([count,1], END)\n","  return tf.concat([starts, ragged, ends], axis=1)\n","\n","class CustomTokenizer(tf.Module):\n","  def __init__(self, reserved_tokens, vocab_path):\n","    self.tokenizer = text.BertTokenizer(vocab_path, lower_case=True)\n","    self._reserved_tokens = reserved_tokens\n","    self._vocab_path = tf.saved_model.Asset(vocab_path)\n","\n","    vocab = pathlib.Path(vocab_path).read_text().splitlines()\n","    self.vocab = tf.Variable(vocab)\n","\n","    ## Create the signatures for export:   \n","\n","    # Include a tokenize signature for a batch of strings. \n","    self.tokenize.get_concrete_function(\n","        tf.TensorSpec(shape=[None], dtype=tf.string))\n","\n","    # Include `detokenize` and `lookup` signatures for:\n","    #   * `Tensors` with shapes [tokens] and [batch, tokens]\n","    #   * `RaggedTensors` with shape [batch, tokens]\n","    self.detokenize.get_concrete_function(\n","        tf.TensorSpec(shape=[None, None], dtype=tf.int64))\n","    self.detokenize.get_concrete_function(\n","          tf.RaggedTensorSpec(shape=[None, None], dtype=tf.int64))\n","\n","    self.lookup.get_concrete_function(\n","        tf.TensorSpec(shape=[None, None], dtype=tf.int64))\n","    self.lookup.get_concrete_function(\n","          tf.RaggedTensorSpec(shape=[None, None], dtype=tf.int64))\n","\n","    # These `get_*` methods take no arguments\n","    self.get_vocab_size.get_concrete_function()\n","    self.get_vocab_path.get_concrete_function()\n","    self.get_reserved_tokens.get_concrete_function()\n","\n","  @tf.function\n","  def tokenize(self, strings):\n","    enc = self.tokenizer.tokenize(strings)\n","    # Merge the `word` and `word-piece` axes.\n","    enc = enc.merge_dims(-2,-1)\n","    enc = add_start_end(enc)\n","    return enc\n","\n","  @tf.function\n","  def detokenize(self, tokenized):\n","    words = self.tokenizer.detokenize(tokenized)\n","    return cleanup_text(self._reserved_tokens, words)\n","\n","  @tf.function\n","  def lookup(self, token_ids):\n","    return tf.gather(self.vocab, token_ids)\n","\n","  @tf.function\n","  def get_vocab_size(self):\n","    return tf.shape(self.vocab)[0]\n","\n","  @tf.function\n","  def get_vocab_path(self):\n","    return self._vocab_path\n","\n","  @tf.function\n","  def get_reserved_tokens(self):\n","    return tf.constant(self._reserved_tokens)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GubNU_HK_dX7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1624123276943,"user_tz":360,"elapsed":3271,"user":{"displayName":"Tim Sullivan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgDOmSXGsobJcA9UOtHRwU-sWjSZffSAqqvuM2IHQ=s64","userId":"11783081476684672772"}},"outputId":"5440e25f-fba1-4d6a-8e89-34f6f2909705"},"source":["tokenizers = tf.Module()\n","tokenizers.ru = CustomTokenizer(reserved_tokens, 'ru_vocab.txt')\n","tokenizers.en = CustomTokenizer(reserved_tokens, 'en_vocab.txt')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/array_ops.py:5049: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n","Instructions for updating:\n","The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/array_ops.py:5049: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n","Instructions for updating:\n","The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n"],"