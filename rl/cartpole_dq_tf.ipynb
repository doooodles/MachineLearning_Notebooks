{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.04873255, 0.00151558, 0.00790912, 0.03975826])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "env.reset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2 possible actions.\n"
     ]
    }
   ],
   "source": [
    "action_space = env.action_space.n\n",
    "print(f\"There are {action_space} possible actions.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory():\n",
    "    def __init__(self, max_size):\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "\n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        buffer_size = len(self.buffer)\n",
    "        index = np.random.choice(np.arange(buffer_size),\n",
    "                                 size=batch_size,\n",
    "                                 replace=False)\n",
    "        return [self.buffer[i] for i in index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 2, 6, 9, 7]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mem = Memory(10)\n",
    "[mem.add(i) for i in np.arange(10)]\n",
    "mem.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MEMORY_SIZE = 600\n",
    "pretrain_length = 10\n",
    "memory = Memory(max_size=MEMORY_SIZE)\n",
    "\n",
    "state = env.reset() #env.decode(env.reset()))\n",
    "\n",
    "done = False\n",
    "step_limit = 600\n",
    "step = 0\n",
    "while step < step_limit:\n",
    "    \n",
    "    random_action = env.action_space.sample()\n",
    "    new_state, reward, done, info = env.step(random_action)\n",
    "    \n",
    "    \n",
    "    memory.add((state, random_action, new_state, reward, done, info))\n",
    "    \n",
    "    if done:\n",
    "        state = env.reset() \n",
    "        \n",
    "    else:\n",
    "        state = new_state\n",
    "        \n",
    "    step += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(array([ 0.16259643,  0.44354756, -0.20166342, -0.99535918]),\n",
       "  0,\n",
       "  array([ 0.17146738,  0.25160983, -0.2215706 , -0.77217508]),\n",
       "  1.0,\n",
       "  True,\n",
       "  {}),\n",
       " (array([ 0.22522695,  1.34652498,  0.0434256 , -0.75930645]),\n",
       "  1,\n",
       "  array([ 0.25215745,  1.54102252,  0.02823947, -1.03801462]),\n",
       "  1.0,\n",
       "  False,\n",
       "  {}),\n",
       " (array([-0.0213989 ,  0.22315138,  0.02583269, -0.04648624]),\n",
       "  0,\n",
       "  array([-0.01693587,  0.02766871,  0.02490296,  0.25423389]),\n",
       "  1.0,\n",
       "  False,\n",
       "  {})]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.sample(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 2), dtype=float32, numpy=\n",
       "array([[-0.08752951,  0.36651555],\n",
       "       [-0.04168215,  0.14394209],\n",
       "       [-0.02554925,  0.16529503]], dtype=float32)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predict the Q value of each action given the state\n",
    "inputs = layers.Input(shape=(4,))\n",
    "x = layers.Dense(50, activation=\"relu\")(inputs)\n",
    "x = layers.Dense(50, activation=\"relu\")(x)\n",
    "outputs = layers.Dense(2, activation=\"linear\")(x)\n",
    "\n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "model_target = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "\n",
    "model.output_shape\n",
    "model(tf.random.uniform((3,4),-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(episode, model, state, min_epsilon, max_epsilon, decay_rate):\n",
    "\n",
    "    # random number for explore/exploit trade-off\n",
    "    epsilon = np.random.rand()\n",
    "\n",
    "    # current ee prob\n",
    "    explore_prob = min_epsilon + (max_epsilon - min_epsilon) * np.exp(-decay_rate*episode)\n",
    "\n",
    "    if epsilon < explore_prob:\n",
    "        action = env.action_space.sample()\n",
    "    else:\n",
    "        q_vals = model(tf.expand_dims(state, axis=0))\n",
    "        action = np.argmax(q_vals)\n",
    "\n",
    "    return action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 0, 1, 1, 1, 1, 1, 1, 1]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = env.reset()\n",
    "[select_action(1, model, tf.random.normal((4,)), 0.0, 0.0, 0.01) for i in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(3,), dtype=float32, numpy=array([1.3121494, 1.060068 , 1.       ], dtype=float32)>,\n",
       " array([[ 0.0277234 ,  0.35306355, -0.01217392, -0.59877353],\n",
       "        [-0.03926528,  0.04129011,  0.03298711, -0.01916819],\n",
       "        [ 0.11184638,  0.18972477, -0.19914572, -0.66553929]]),\n",
       " [1, 0, 1])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_minibatch(model, memory, gamma, batch_size):\n",
    "\n",
    "    # memory structure: (state, action, new_state, reward, done, info)\n",
    "\n",
    "    batch = memory.sample(batch_size)\n",
    "    states = np.array([each[0] for each in batch])\n",
    "    actions =[each[1] for each in batch]\n",
    "    next_states = np.array([each[2] for each in batch])\n",
    "    rewards = [each[3] for each in batch]\n",
    "    dones = tf.constant([each[4] for each in batch], dtype=tf.float32)\n",
    "\n",
    "    # get q values from target model\n",
    "    q_target = model(next_states)\n",
    "    \n",
    "    q_target = tf.reduce_max(q_target, axis=1)\n",
    "    # set done q_target = reward and discount the others\n",
    "    q_target = rewards + gamma * (1. - dones) * q_target\n",
    "\n",
    "    return q_target, states, actions\n",
    "\n",
    "get_minibatch(model, memory, 0.99, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_episodes = 1000       # Total number of training episodes\n",
    "max_steps = 200               # Max steps per episode\n",
    "batch_size = 64\n",
    "\n",
    "learning_rate = 0.01          # Learning rate\n",
    "gamma = 0.9                  # Discounting rate\n",
    "\n",
    "# Exploration parameters\n",
    "epsilon = 1.0               # Exploration rate\n",
    "max_epsilon = 1             # Exploration probability at start\n",
    "min_epsilon = .1            # Minimum exploration probability \n",
    "decay_rate = 0.003          # Exponential decay rate for exploration prob\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Il7ecAAAACXBIWXMAAAsTAAALEwEAmpwYAAAjuUlEQVR4nO3deXhV1b3/8fc3JyHzBAkhkDATICAIRAYnggOCVWkdUKyzFLVy61Tv1XpbrbX9abXO2oLVWkecUBERelEiagEBZR7DPBoIEEgYQ9bvj3O0EUFCOMnOOefzep48OXvvdTjflZ3nw87aa+9tzjlERCT0RXldgIiIBIcCXUQkTCjQRUTChAJdRCRMKNBFRMJEtFcfnJGR4Vq3bl2r91ZUVJCYmBjcgho49TkyqM+R4Xj6PHv27K3OuczDbfMs0Fu3bs2sWbNq9d6ioiIKCwuDW1ADpz5HBvU5MhxPn81szZG2achFRCRMKNBFRMKEAl1EJEwo0EVEwoQCXUQkTBw10M3sBTMrMbMFR9huZvakmRWb2Twz6xn8MkVE5GhqcoT+IjDoR7YPBjoEvkYAfz3+skRE5FgdNdCdc1OBbT/SZAjwkvObDqSZWXawCjzUgg1lvLV0P7rtr4jI9wXjwqIWwLpqy+sD6zYd2tDMRuA/iicrK4uioqJj/rDJaw7w4aoDdBn7CflNfLUqOBSVl5fX6ucVytTnyKA+B0+9XinqnBsNjAYoKChwtblSqu+Bg4x/YBJTtsRz04X9MLMgV9kw6Wq6yKA+R4a66nMwZrlsAHKrLecE1tWJuBgf57WNYebq7fx7RWldfYyISMgJRqCPA64KzHbpC5Q5534w3BJMp+dE0ywljscnL9NYuohI